{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code\")\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code/thesis_code\")\n",
    "\n",
    "import os, glob\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfl = tf.keras.layers\n",
    "tfq = tf.quantization\n",
    "\n",
    "from binary_io import to_bit_string, from_bit_string\n",
    "\n",
    "from architectures import ProbabilisticLadderNetwork, VariationalAutoEncoder\n",
    "\n",
    "from miracle import create_dataset, quantize_image, read_png\n",
    "\n",
    "from greedy_compression import code_grouped_greedy_sample, code_grouped_importance_sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pln_code_image_greedy(image, \n",
    "                          latent_dist_dir,\n",
    "                          latent_dist_format,\n",
    "                          seed, \n",
    "                          n_steps,\n",
    "                          n_bits_per_step,\n",
    "                          comp_file_path,\n",
    "                          backfitting_steps_level_1=0,\n",
    "                          backfitting_steps_level_2=0,\n",
    "                          use_log_prob=False,\n",
    "                          rho=1.,\n",
    "                          use_importance_sampling=True,\n",
    "                          first_level_max_group_size_bits=12,\n",
    "                          second_level_n_bits_per_group=20,\n",
    "                          second_level_max_group_size_bits=4,\n",
    "                          second_level_dim_kl_bit_limit=12,\n",
    "                          outlier_index_bytes=3,\n",
    "                          outlier_sample_bytes=2,\n",
    "                          verbose=False):\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 1: Set the latent distributions for the image\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        q1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q1_loc\"))\n",
    "        q1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q1_scale\"))\n",
    "\n",
    "        p1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p1_loc\"))\n",
    "        p1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p1_scale\"))\n",
    "\n",
    "        q2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q2_loc\"))\n",
    "        q2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q2_scale\"))\n",
    "\n",
    "        p2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p2_loc\"))\n",
    "        p2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p2_scale\"))\n",
    "        \n",
    "        q1 = tfd.Normal(loc=q1_loc,\n",
    "                        scale=q1_scale)\n",
    "        \n",
    "        p1 = tfd.Normal(loc=p1_loc,\n",
    "                        scale=p1_scale)\n",
    "        \n",
    "        q2 = tfd.Normal(loc=q2_loc,\n",
    "                        scale=q2_scale)\n",
    "        \n",
    "        p2 = tfd.Normal(loc=p2_loc,\n",
    "                        scale=p2_scale)\n",
    "        \n",
    "        first_level_shape = q1.loc.shape.as_list()\n",
    "        second_level_shape = q2.loc.shape.as_list()\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 2: Create a coded sample of the latent space\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        if verbose: print(\"Coding second level\")\n",
    "            \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            sample2, code2, group_indices2, outlier_extras2 = code_grouped_importance_sample(\n",
    "                target=q2, \n",
    "                proposal=p2, \n",
    "                n_bits_per_group=second_level_n_bits_per_group, \n",
    "                seed=seed, \n",
    "                max_group_size_bits=second_level_max_group_size_bits,\n",
    "                dim_kl_bit_limit=second_level_dim_kl_bit_limit)\n",
    "            \n",
    "            outlier_extras2 = list(map(lambda x: tf.reshape(x, [-1]).numpy(), outlier_extras2))\n",
    "            \n",
    "        else:\n",
    "            sample2, code2, group_indices2 = code_grouped_greedy_sample(target=q2, \n",
    "                                                                        proposal=p2, \n",
    "                                                                        n_bits_per_step=n_bits_per_step, \n",
    "                                                                        n_steps=n_steps, \n",
    "                                                                        seed=seed, \n",
    "                                                                        max_group_size_bits=second_level_max_group_size_bits,\n",
    "                                                                        adaptive=True,\n",
    "                                                                        backfitting_steps=backfitting_steps_level_2,\n",
    "                                                                        use_log_prob=use_log_prob,\n",
    "                                                                        rho=rho)\n",
    "            \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences2 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices2)):\n",
    "            group_differences2.append(group_indices2[i] - group_indices2[i - 1])\n",
    "        \n",
    "        # We need to adjust the priors to the second stage sample\n",
    "        latents = (tf.reshape(sample2, second_level_shape), latents[1])\n",
    "        \n",
    "        \n",
    "        # Calculate the priors\n",
    "        self.decode(latents)\n",
    "        \n",
    "        if verbose: print(\"Coding first level\")\n",
    "            \n",
    "        sample1, code1, group_indices1 = code_grouped_greedy_sample(target=self.latent_posteriors[0], \n",
    "                                                                    proposal=self.latent_priors[0], \n",
    "                                                                    n_bits_per_step=n_bits_per_step, \n",
    "                                                                    n_steps=n_steps, \n",
    "                                                                    seed=seed, \n",
    "                                                                    max_group_size_bits=first_level_max_group_size_bits,\n",
    "                                                                    backfitting_steps=backfitting_steps_level_1,\n",
    "                                                                    use_log_prob=use_log_prob,\n",
    "                                                                    adaptive=True)\n",
    "        \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences1 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices1)):\n",
    "            group_differences1.append(group_indices1[i] - group_indices1[i - 1])\n",
    "        \n",
    "        bitcode = code1 + code2\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 3: Write the compressed file\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        extras = [seed, n_steps, n_bits_per_step] + first_level_shape[1:3] + second_level_shape[1:3]\n",
    "        \n",
    "        var_length_extras = [group_differences1, group_differences2]\n",
    "        var_length_bits = [first_level_max_group_size_bits,  \n",
    "                           second_level_max_group_size_bits]\n",
    "        \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            var_length_extras += outlier_extras2\n",
    "            var_length_bits += [ outlier_index_bytes * 8, outlier_sample_bytes * 8 ]\n",
    "    \n",
    "        write_bin_code(bitcode, \n",
    "                       comp_file_path, \n",
    "                       extras=extras,\n",
    "                       var_length_extras=var_length_extras,\n",
    "                       var_length_bits=var_length_bits)\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 4: Some logging information\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        total_kls = [tf.reduce_sum(x) for x in self.kl_divergence]\n",
    "        total_kl = sum(total_kls)\n",
    "\n",
    "        theoretical_byte_size = (total_kl + 2 * np.log(total_kl + 1)) / np.log(2) / 8\n",
    "        extra_byte_size = len(group_indices1) * var_length_bits[0] // 8 + \\\n",
    "                          len(group_indices2) * var_length_bits[1] // 8 + 7 * 2\n",
    "        actual_byte_size = os.path.getsize(comp_file_path)\n",
    "\n",
    "        actual_no_extra = actual_byte_size - extra_byte_size\n",
    "        \n",
    "        first_level_theoretical = (total_kls[0] + 2 * np.log(total_kls[0] + 1)) / np.log(2) / 8\n",
    "        first_level_actual_no_extra = len(code1) / 8\n",
    "        first_level_extra = len(group_indices1) * var_length_bits[0] // 8\n",
    "\n",
    "        sample1_reshaped = tf.reshape(sample1, first_level_shape)\n",
    "        first_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[0].log_prob(sample1_reshaped))\n",
    "        first_level_sample_avg = tf.reduce_mean(self.latent_posteriors[0].log_prob(self.latent_posteriors[0].sample()))\n",
    "        \n",
    "        second_level_theoretical = (total_kls[1] + 2 * np.log(total_kls[1] + 1)) / np.log(2) / 8\n",
    "        second_level_actual_no_extra = len(code2) / 8\n",
    "        second_level_extra = len(group_indices2) * var_length_bits[1] // 8 + 1\n",
    "        \n",
    "        second_bpp = (second_level_actual_no_extra + second_level_extra) * 8 / (image_shape[1] * image_shape[2]) \n",
    "\n",
    "        sample2_reshaped = tf.reshape(sample2, second_level_shape)\n",
    "        second_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[1].log_prob(sample2_reshaped))\n",
    "        second_level_sample_avg = tf.reduce_mean(self.latent_posteriors[1].log_prob(self.latent_posteriors[1].sample()))\n",
    "        \n",
    "        bpp = 8 * actual_byte_size / (image_shape[1] * image_shape[2]) \n",
    "        \n",
    "        summaries = {\n",
    "            \"image_shape\": image_shape,\n",
    "            \"theoretical_byte_size\": float(theoretical_byte_size.numpy()),\n",
    "            \"actual_byte_size\": actual_byte_size,\n",
    "            \"extra_byte_size\": extra_byte_size,\n",
    "            \"actual_no_extra\": actual_no_extra,\n",
    "            \"second_bpp\": second_bpp,\n",
    "            \"bpp\": bpp\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "\n",
    "            print(\"Image dimensions: {}\".format(image_shape))\n",
    "            print(\"Theoretical size: {:.2f} bytes\".format(theoretical_byte_size))\n",
    "            print(\"Actual size: {:.2f} bytes\".format(actual_byte_size))\n",
    "            print(\"Extra information size: {:.2f} bytes {:.2f}% of actual size\".format(extra_byte_size, \n",
    "                                                                                       100 * extra_byte_size / actual_byte_size))\n",
    "            print(\"Actual size without extras: {:.2f} bytes\".format(actual_no_extra))\n",
    "            print(\"Efficiency: {:.3f}\".format(actual_byte_size / theoretical_byte_size))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"First level theoretical size: {:.2f} bytes\".format(first_level_theoretical))\n",
    "            print(\"First level actual (no extras) size: {:.2f} bytes\".format(first_level_actual_no_extra))\n",
    "            print(\"First level extras size: {:.2f} bytes\".format(first_level_extra))\n",
    "            print(\"First level Efficiency: {:.3f}\".format(\n",
    "                (first_level_actual_no_extra + first_level_extra) / first_level_theoretical))\n",
    "            \n",
    "            print(\"First level # of groups: {}\".format(len(group_indices1)))\n",
    "            print(\"First level greedy sample average log likelihood: {:.4f}\".format(first_level_avg_log_lik))\n",
    "            print(\"First level average sample log likelihood on level 1: {:.4f}\".format(first_level_sample_avg))\n",
    "            print(\"\")\n",
    "           \n",
    "            print(\"Second level theoretical size: {:.2f} bytes\".format(second_level_theoretical))\n",
    "            print(\"Second level actual (no extras) size: {:.2f} bytes\".format(second_level_actual_no_extra))\n",
    "            print(\"Second level extras size: {:.2f} bytes\".format(second_level_extra))\n",
    "\n",
    "            if use_importance_sampling:\n",
    "                print(\"{} outliers were not compressed (higher than {} bits of KL)\".format(len(outlier_extras2[0]),\n",
    "                                                                                           second_level_dim_kl_bit_limit))\n",
    "            print(\"Second level Efficiency: {:.3f}\".format(\n",
    "                (second_level_actual_no_extra + second_level_extra) / second_level_theoretical))\n",
    "            print(\"Second level's contribution to bpp: {:.4f}\".format(second_bpp))\n",
    "            print(\"Second level # of groups: {}\".format(len(group_indices2)))\n",
    "            print(\"Second level greedy sample average log likelihood: {:.4f}\".format(second_level_avg_log_lik))\n",
    "            print(\"Second level average sample log likelihood on level 1: {:.4f}\".format(second_level_sample_avg))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"{:.4f} bits / pixel\".format( bpp ))\n",
    "        \n",
    "        return (sample2, sample1), summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "q1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q1_loc\")).flatten()[:100]\n",
    "q1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q1_scale\")).flatten()[:100]\n",
    "\n",
    "p1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p1_loc\")).flatten()[:100]\n",
    "p1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p1_scale\")).flatten()[:100]\n",
    "\n",
    "q2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q2_loc\")).flatten()\n",
    "q2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q2_scale\")).flatten()\n",
    "\n",
    "p2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p2_loc\")).flatten()\n",
    "p2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p2_scale\")).flatten()\n",
    "\n",
    "q1 = tfd.Normal(loc=q1_loc,\n",
    "                scale=q1_scale)\n",
    "\n",
    "p1 = tfd.Normal(loc=p1_loc,\n",
    "                scale=p1_scale)\n",
    "\n",
    "q2 = tfd.Normal(loc=q2_loc,\n",
    "                scale=q2_scale)\n",
    "\n",
    "p2 = tfd.Normal(loc=p2_loc,\n",
    "                scale=p2_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_importance_sample_(sess,\n",
    "                           target,\n",
    "                           proposal,\n",
    "                           n_coding_bits,\n",
    "                           seed):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    #print(\"Taking {} samples per step\".format(n_samples))\n",
    "    \n",
    "    sample_index = []\n",
    "    \n",
    "    kls = tfd.kl_divergence(target, proposal)\n",
    "    total_kl = tf.reduce_sum(kls)\n",
    "    \n",
    "    num_samples = tf.cast(tf.ceil(tf.exp(total_kl)), tf.int32)\n",
    "    \n",
    "    # Set new seed\n",
    "    #tf.random.set_random_seed(seed)\n",
    "    samples = proposal.sample(num_samples, seed=seed)\n",
    "\n",
    "    importance_weights = tf.reduce_sum(target.log_prob(samples) - proposal.log_prob(samples), axis=1)\n",
    "\n",
    "    idx = tf.argmax(importance_weights)\n",
    "    best_samp = samples[idx:idx + 1, :]\n",
    "    \n",
    "    index, best_sample = sess.run([idx, best_samp])\n",
    "    \n",
    "    if np.log(index + 1) / np.log(2) > n_coding_bits:\n",
    "        raise Exception(\"Not enough bits to code importance sample!\")\n",
    "    \n",
    "    bitcode = to_bit_string(index, n_coding_bits)\n",
    "\n",
    "    return best_sample, bitcode\n",
    "\n",
    "\n",
    "def decode_importance_sample_(sess, sample_index, proposal, seed):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    index = from_bit_string(sample_index)\n",
    "    \n",
    "    #tf.random.set_random_seed(seed)\n",
    "    samples = proposal.sample(index + 1, seed=seed)\n",
    "    \n",
    "    return sess.run(samples[index:, ...])\n",
    "\n",
    "\n",
    "def code_grouped_importance_sample_(sess,\n",
    "                                    target, \n",
    "                                    proposal, \n",
    "                                    seed,\n",
    "                                    n_bits_per_group,\n",
    "                                    max_group_size_bits=4,\n",
    "                                    dim_kl_bit_limit=12):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "        \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1])\n",
    "    t_scale = tf.reshape(target.scale / proposal.scale, [-1])\n",
    "    \n",
    "    # If we're going to do importance sampling, separate out dimensions with large KL,\n",
    "    # we'll deal with them separately.\n",
    "    kl_bits = tf.reshape(tfd.kl_divergence(target, proposal), [-1]) / np.log(2)\n",
    "\n",
    "    t_loc = tf.where(kl_bits <= dim_kl_bit_limit, t_loc, p_loc)\n",
    "    t_scale = tf.where(kl_bits <= dim_kl_bit_limit, t_scale, p_scale)\n",
    "\n",
    "    # We'll send the quantized samples for dimensions with high KL\n",
    "    outlier_indices = tf.where(kl_bits > dim_kl_bit_limit)\n",
    "\n",
    "    target_samples = tf.reshape(target.sample(), [-1])\n",
    "\n",
    "    # Select only the bits of the sample that are relevant\n",
    "    outlier_samples = tf.gather_nd(target_samples, outlier_indices)\n",
    "\n",
    "    # Halve precision\n",
    "    outlier_samples = tfq.quantize(outlier_samples, -30, 30, tf.quint16).output\n",
    "\n",
    "    outlier_extras = (outlier_indices, outlier_samples)\n",
    "    \n",
    "    kl_divergences = tf.reshape(\n",
    "        tfd.kl_divergence(tfd.Normal(loc=t_loc, scale=t_scale), \n",
    "                          tfd.Normal(loc=p_loc, scale=p_scale)), [-1])\n",
    "\n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "        \n",
    "    print(\"Maximum group KL: {:.3f}\".format(np.max(group_kls)))\n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        result = code_importance_sample_(\n",
    "            sess,\n",
    "            target=tfd.Normal(loc=t_loc[start_idx:end_idx],\n",
    "                              scale=t_scale[start_idx:end_idx]), \n",
    "\n",
    "            proposal=tfd.Normal(loc=p_loc[start_idx:end_idx],\n",
    "                                scale=p_scale[start_idx:end_idx]), \n",
    "\n",
    "            n_coding_bits=n_bits_per_group,\n",
    "            seed=i + seed)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = ''.join(codes)\n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = tf.where(kl_bits <= dim_kl_bit_limit, tf.squeeze(sample), target_samples)\n",
    "    \n",
    "    sample, outlier_extras = sess.run([sample, outlier_extras])\n",
    "    \n",
    "    return sample, bitcode, group_start_indices, outlier_extras\n",
    "\n",
    "\n",
    "def decode_grouped_importance_sample_(sess,\n",
    "                                     bitcode, \n",
    "                                     group_start_indices,\n",
    "                                     proposal, \n",
    "                                     n_bits_per_group,\n",
    "                                     seed,\n",
    "                                     outlier_indices,\n",
    "                                     outlier_samples):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "\n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        samples.append(decode_importance_sample_(\n",
    "            sess,\n",
    "            sample_index=bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            \n",
    "            proposal=tfd.Normal(loc=p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "                                scale=p_scale[group_start_indices[i]:group_start_indices[i + 1]]), \n",
    "            seed=i + seed))\n",
    "        \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    sample = tf.squeeze(sample)\n",
    "    \n",
    "    # Dequantize outliers\n",
    "    outlier_samples = tfq.dequantize(tf.cast(outlier_samples, tf.quint16), -30, 30)\n",
    "    \n",
    "    # Add back the quantized outliers\n",
    "    outlier_indices = tf.cast(tf.reshape(outlier_indices, [-1, 1]), tf.int32)\n",
    "    outlier_samples = tf.reshape(outlier_samples, [-1])\n",
    "    \n",
    "    updates = tf.scatter_nd(outlier_indices, \n",
    "                            outlier_samples, \n",
    "                            sample.shape.as_list())\n",
    "                            \n",
    "    sample = tf.where(tf.equal(updates, 0), sample, updates)\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_greedy_sample_(sess,\n",
    "                       target, \n",
    "                       proposal, \n",
    "                       n_bits_per_step, \n",
    "                       n_steps, \n",
    "                       seed, \n",
    "                       rho=1., \n",
    "                       backfitting_steps=0,\n",
    "                       use_log_prob=False):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "    \n",
    "    #print(\"Taking {} samples per step\".format(n_samples))\n",
    "\n",
    "    best_sample = tf.Variable(tf.zeros((1, dim), dtype=tf.float32))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_index = []\n",
    "    \n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)\n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=proposal.loc / n_steps,\n",
    "                                scale=rho * proposal.scale / scale_divisor)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # Set new seed\n",
    "        samples = proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "\n",
    "        test_samples = tf.tile(best_sample, [n_samples, 1]) + samples\n",
    "\n",
    "        log_probs = tf.reduce_sum(target.log_prob(test_samples), axis=1)\n",
    "\n",
    "        index = tf.argmax(log_probs)\n",
    "\n",
    "        update_samp_op = best_sample.assign(test_samples[index:index + 1, :])\n",
    "\n",
    "        idx, _ = sess.run([index, update_samp_op])\n",
    "        \n",
    "        sample_index.append(idx)\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Perform backfitting\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # TODO\n",
    "#     for b in range(backfitting_steps):\n",
    "        \n",
    "#         # Single backfitting step\n",
    "#         for i in range(n_steps):\n",
    "\n",
    "#             # Set seed to regenerate the previously generated samples here\n",
    "#             samples = proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "            \n",
    "#             idx = sample_index[i]\n",
    "            \n",
    "#             # Undo the addition of the current sample\n",
    "#             best_sample.assign_sub(samples[idx : idx + 1, :])\n",
    "            \n",
    "#             # Generate candidate samples\n",
    "#             test_samples = tf.tile(best_sample, [n_samples, 1]) + samples\n",
    "\n",
    "#             if use_log_prob:\n",
    "#                 test_scores = tf.reduce_sum(target.log_prob(test_samples), axis=1)\n",
    "#             else:\n",
    "#                 test_scores = tf.reduce_sum(-((test_samples - target.loc) / target.scale)**8,\n",
    "#                                            axis=1)\n",
    "\n",
    "#             index = tf.argmax(test_scores)\n",
    "\n",
    "#             samp_update_op = best_sample.assign(test_samples[index:index + 1, :])\n",
    "\n",
    "#             idx, _ = sess.run([index, samp_update_op])\n",
    "            \n",
    "#             sample_index[i] = idx\n",
    "    \n",
    "    \n",
    "    sample_index = list(map(lambda x: to_bit_string(x, n_bits_per_step), sample_index))\n",
    "    sample_index = ''.join(sample_index)\n",
    "    \n",
    "    return best_sample.eval(session=sess), sample_index\n",
    "\n",
    "\n",
    "\n",
    "def decode_greedy_sample_(sess, sample_index, proposal, n_bits_per_step, n_steps, seed, rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    indices = [from_bit_string(sample_index[i:i + n_bits_per_step]) \n",
    "               for i in range(0, n_bits_per_step * n_steps, n_bits_per_step)]\n",
    "        \n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)    \n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=proposal.loc / n_steps,\n",
    "                                scale=rho * proposal.scale / scale_divisor)    \n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "    \n",
    "    sample = tf.Variable(tf.zeros((1, dim), dtype=tf.float32))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        # Set new seed\n",
    "        samples = tf.tile(sample, [n_samples, 1]) + proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "\n",
    "        index = indices[i]\n",
    "\n",
    "        samp_update_op = sample.assign(samples[index:index + 1, :])\n",
    "        \n",
    "        sess.run(samp_update_op)\n",
    "    \n",
    "    return sample.eval(session=sess)\n",
    "\n",
    "\n",
    "def code_grouped_greedy_sample_(sess,\n",
    "                                target, \n",
    "                               proposal,\n",
    "                               n_steps, \n",
    "                               n_bits_per_step,\n",
    "                               seed,\n",
    "                               max_group_size_bits=12,\n",
    "                               adaptive=True,\n",
    "                               backfitting_steps=0,\n",
    "                               use_log_prob=False,\n",
    "                               rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1])\n",
    "    t_scale = tf.reshape(target.scale / proposal.scale, [-1])\n",
    "    \n",
    "    kl_divergences = tf.reshape(tfd.kl_divergence(target, proposal), [-1])\n",
    "        \n",
    "    # ====================================================================== \n",
    "    # Preprocessing step: determine groups for sampling\n",
    "    # ====================================================================== \n",
    "\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "    \n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "            \n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        result = code_greedy_sample_(\n",
    "            sess=sess,\n",
    "            target=tfd.Normal(loc=t_loc[start_idx:end_idx],\n",
    "                              scale=t_scale[start_idx:end_idx]), \n",
    "\n",
    "            proposal=tfd.Normal(loc=p_loc[start_idx:end_idx],\n",
    "                                scale=p_scale[start_idx:end_idx]), \n",
    "\n",
    "            n_bits_per_step=n_bits_per_step, \n",
    "            n_steps=n_steps, \n",
    "            seed=i + seed,\n",
    "            backfitting_steps=backfitting_steps,\n",
    "            use_log_prob=use_log_prob,\n",
    "            rho=rho)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = ''.join(codes)\n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = sess.run(sample)\n",
    "    \n",
    "    return sample, bitcode, group_start_indices\n",
    "  \n",
    "    \n",
    "def decode_grouped_greedy_sample_(sess,\n",
    "                                  bitcode, \n",
    "                                 group_start_indices,\n",
    "                                 proposal, \n",
    "                                 n_bits_per_step, \n",
    "                                 n_steps, \n",
    "                                 seed,\n",
    "                                 adaptive=True,\n",
    "                                 rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        samples.append(decode_greedy_sample_(\n",
    "            sess=sess,\n",
    "            sample_index=bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            \n",
    "            proposal=tfd.Normal(loc=p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "                                scale=p_scale[group_start_indices[i]:group_start_indices[i + 1]]), \n",
    "            \n",
    "            n_bits_per_step=n_bits_per_step, \n",
    "            n_steps=n_steps, \n",
    "            seed=i + seed,\n",
    "            rho=rho))\n",
    "        \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total KL to split up: 2127.70 bits, maximum bits per group: 420, estimated number of groups: 6.0,coding 100 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:35<00:00,  6.27s/it]\n",
      "100%|██████████| 8/8 [01:48<00:00, 15.05s/it]\n"
     ]
    }
   ],
   "source": [
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "# # This is so that setting the seed actually does something\n",
    "# #tf.reset_default_graph()\n",
    "\n",
    "# q1 = tfd.Normal(loc=q1_loc,\n",
    "#                 scale=q1_scale)\n",
    "\n",
    "# p1 = tfd.Normal(loc=p1_loc,\n",
    "#                 scale=p1_scale)\n",
    "\n",
    "# q2 = tfd.Normal(loc=q2_loc,\n",
    "#                 scale=q2_scale)\n",
    "\n",
    "# p2 = tfd.Normal(loc=p2_loc,\n",
    "#                 scale=p2_scale)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "#     res = code_grouped_importance_sample_(sess=sess,\n",
    "#                                             target=q2, \n",
    "#                                             proposal=p2, \n",
    "#                                             seed=seed,\n",
    "#                                             n_bits_per_group=20,\n",
    "#                                             max_group_size_bits=4,\n",
    "#                                             dim_kl_bit_limit=12)\n",
    "    \n",
    "#     sample, bitcode, group_start_indices, outlier_extras = res\n",
    "    \n",
    "#     decoded = decode_grouped_importance_sample_(sess=sess,\n",
    "#                                                  bitcode=bitcode, \n",
    "#                                                  group_start_indices=group_start_indices,\n",
    "#                                                  proposal=p2, \n",
    "#                                                  n_bits_per_group=20,\n",
    "#                                                  seed=seed,\n",
    "#                                                  outlier_indices=outlier_extras[0],\n",
    "#                                                  outlier_samples=outlier_extras[1])\n",
    "\n",
    "    res = code_grouped_greedy_sample_(sess=sess,\n",
    "                                    target=q1, \n",
    "                                   proposal=p1,\n",
    "                                   n_steps=n_steps, \n",
    "                                   n_bits_per_step=n_bits_per_step,\n",
    "                                   seed=seed,\n",
    "                                   max_group_size_bits=12,\n",
    "                                   adaptive=True,\n",
    "                                   backfitting_steps=0,\n",
    "                                   use_log_prob=False,\n",
    "                                   rho=1.)\n",
    "    \n",
    "    sample, bitcode, group_start_indices = res\n",
    "    \n",
    "    dec = decode_grouped_greedy_sample_(sess=sess,\n",
    "                                  bitcode=bitcode, \n",
    "                                 group_start_indices=group_start_indices,\n",
    "                                 proposal=p1, \n",
    "                                 n_bits_per_step=n_bits_per_step, \n",
    "                                 n_steps=n_steps, \n",
    "                                 seed=seed,\n",
    "                                 adaptive=True,\n",
    "                                 rho=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample - dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "pln_code_image_greedy(image=None, \n",
    "                      latent_dist_dir=latent_dist_dir,\n",
    "                      latent_dist_format=latent_dist_format,\n",
    "                      seed=seed, \n",
    "                      n_steps=n_steps,\n",
    "                      n_bits_per_step=n_bits_per_step,\n",
    "                      comp_file_path=comp_file_path,\n",
    "                      backfitting_steps_level_1=0,\n",
    "                      backfitting_steps_level_2=0,\n",
    "                      use_log_prob=False,\n",
    "                      rho=rho,\n",
    "                      use_importance_sampling=True,\n",
    "                      first_level_max_group_size_bits=first_level_max_group_size_bits,\n",
    "                      second_level_n_bits_per_group=20,\n",
    "                      second_level_max_group_size_bits=second_level_max_group_size_bits,\n",
    "                      second_level_dim_kl_bit_limit=12,\n",
    "                      outlier_index_bytes=3,\n",
    "                      outlier_sample_bytes=2,\n",
    "                      verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
