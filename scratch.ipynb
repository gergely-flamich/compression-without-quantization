{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code\")\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code/thesis_code\")\n",
    "\n",
    "import os, glob\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfl = tf.keras.layers\n",
    "tfq = tf.quantization\n",
    "\n",
    "from binary_io import to_bit_string, from_bit_string\n",
    "\n",
    "from architectures import ProbabilisticLadderNetwork, VariationalAutoEncoder\n",
    "\n",
    "from miracle import create_dataset, quantize_image, read_png\n",
    "\n",
    "from greedy_compression import code_grouped_greedy_sample, code_grouped_importance_sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pln_code_image_greedy(session,\n",
    "                          image, \n",
    "                          seed, \n",
    "                          n_steps,\n",
    "                          n_bits_per_step,\n",
    "                          comp_file_path,\n",
    "                          backfitting_steps_level_1=0,\n",
    "                          backfitting_steps_level_2=0,\n",
    "                          use_log_prob=False,\n",
    "                          rho=1.,\n",
    "                          use_importance_sampling=True,\n",
    "                          first_level_max_group_size_bits=12,\n",
    "                          second_level_n_bits_per_group=20,\n",
    "                          second_level_max_group_size_bits=4,\n",
    "                          second_level_dim_kl_bit_limit=12,\n",
    "                          outlier_index_bytes=3,\n",
    "                          outlier_sample_bytes=2,\n",
    "                          verbose=False):\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 1: Set the latent distributions for the image\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        sess.run(self)\n",
    "        \n",
    "        first_level_shape = self.posterior_1.loc.shape.as_list()\n",
    "        second_level_shape = self.posterior_2.loc.shape.as_list()\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 2: Create a coded sample of the latent space\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        if verbose: print(\"Coding second level\")\n",
    "            \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            sample2, code2, group_indices2, outlier_extras2 = code_grouped_importance_sample(\n",
    "                sess=session,\n",
    "                target=self.posterior_2,\n",
    "                proposal=self.prior_2, \n",
    "                n_bits_per_group=second_level_n_bits_per_group, \n",
    "                seed=seed, \n",
    "                max_group_size_bits=second_level_max_group_size_bits,\n",
    "                dim_kl_bit_limit=second_level_dim_kl_bit_limit)\n",
    "            \n",
    "            outlier_extras2 = list(map(lambda x: tf.reshape(x, [-1]).numpy(), outlier_extras2))\n",
    "            \n",
    "        else:\n",
    "            sample2, code2, group_indices2 = code_grouped_greedy_sample_(sess=session,\n",
    "                                                                        target=self.posterior_2, \n",
    "                                                                        proposal=self.prior_2, \n",
    "                                                                        n_bits_per_step=n_bits_per_step, \n",
    "                                                                        n_steps=n_steps, \n",
    "                                                                        seed=seed, \n",
    "                                                                        max_group_size_bits=second_level_max_group_size_bits,\n",
    "                                                                        adaptive=True,\n",
    "                                                                        backfitting_steps=backfitting_steps_level_2,\n",
    "                                                                        use_log_prob=use_log_prob,\n",
    "                                                                        rho=rho)\n",
    "            \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences2 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices2)):\n",
    "            group_differences2.append(group_indices2[i] - group_indices2[i - 1])\n",
    "        \n",
    "        # We need to adjust the priors to the second stage sample\n",
    "        latents = (tf.reshape(sample2, second_level_shape), latents[1])\n",
    "        \n",
    "        \n",
    "        # Calculate the priors\n",
    "        self.decode(latents)\n",
    "        \n",
    "        if verbose: print(\"Coding first level\")\n",
    "            \n",
    "        sample1, code1, group_indices1 = code_grouped_greedy_sample(target=self.latent_posteriors[0], \n",
    "                                                                    proposal=self.latent_priors[0], \n",
    "                                                                    n_bits_per_step=n_bits_per_step, \n",
    "                                                                    n_steps=n_steps, \n",
    "                                                                    seed=seed, \n",
    "                                                                    max_group_size_bits=first_level_max_group_size_bits,\n",
    "                                                                    backfitting_steps=backfitting_steps_level_1,\n",
    "                                                                    use_log_prob=use_log_prob,\n",
    "                                                                    adaptive=True)\n",
    "        \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences1 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices1)):\n",
    "            group_differences1.append(group_indices1[i] - group_indices1[i - 1])\n",
    "        \n",
    "        bitcode = code1 + code2\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 3: Write the compressed file\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        extras = [seed, n_steps, n_bits_per_step] + first_level_shape[1:3] + second_level_shape[1:3]\n",
    "        \n",
    "        var_length_extras = [group_differences1, group_differences2]\n",
    "        var_length_bits = [first_level_max_group_size_bits,  \n",
    "                           second_level_max_group_size_bits]\n",
    "        \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            var_length_extras += outlier_extras2\n",
    "            var_length_bits += [ outlier_index_bytes * 8, outlier_sample_bytes * 8 ]\n",
    "    \n",
    "        write_bin_code(bitcode, \n",
    "                       comp_file_path, \n",
    "                       extras=extras,\n",
    "                       var_length_extras=var_length_extras,\n",
    "                       var_length_bits=var_length_bits)\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 4: Some logging information\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        total_kls = [tf.reduce_sum(x) for x in self.kl_divergence]\n",
    "        total_kl = sum(total_kls)\n",
    "\n",
    "        theoretical_byte_size = (total_kl + 2 * np.log(total_kl + 1)) / np.log(2) / 8\n",
    "        extra_byte_size = len(group_indices1) * var_length_bits[0] // 8 + \\\n",
    "                          len(group_indices2) * var_length_bits[1] // 8 + 7 * 2\n",
    "        actual_byte_size = os.path.getsize(comp_file_path)\n",
    "\n",
    "        actual_no_extra = actual_byte_size - extra_byte_size\n",
    "        \n",
    "        first_level_theoretical = (total_kls[0] + 2 * np.log(total_kls[0] + 1)) / np.log(2) / 8\n",
    "        first_level_actual_no_extra = len(code1) / 8\n",
    "        first_level_extra = len(group_indices1) * var_length_bits[0] // 8\n",
    "\n",
    "        sample1_reshaped = tf.reshape(sample1, first_level_shape)\n",
    "        first_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[0].log_prob(sample1_reshaped))\n",
    "        first_level_sample_avg = tf.reduce_mean(self.latent_posteriors[0].log_prob(self.latent_posteriors[0].sample()))\n",
    "        \n",
    "        second_level_theoretical = (total_kls[1] + 2 * np.log(total_kls[1] + 1)) / np.log(2) / 8\n",
    "        second_level_actual_no_extra = len(code2) / 8\n",
    "        second_level_extra = len(group_indices2) * var_length_bits[1] // 8 + 1\n",
    "        \n",
    "        second_bpp = (second_level_actual_no_extra + second_level_extra) * 8 / (image_shape[1] * image_shape[2]) \n",
    "\n",
    "        sample2_reshaped = tf.reshape(sample2, second_level_shape)\n",
    "        second_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[1].log_prob(sample2_reshaped))\n",
    "        second_level_sample_avg = tf.reduce_mean(self.latent_posteriors[1].log_prob(self.latent_posteriors[1].sample()))\n",
    "        \n",
    "        bpp = 8 * actual_byte_size / (image_shape[1] * image_shape[2]) \n",
    "        \n",
    "        summaries = {\n",
    "            \"image_shape\": image_shape,\n",
    "            \"theoretical_byte_size\": float(theoretical_byte_size.numpy()),\n",
    "            \"actual_byte_size\": actual_byte_size,\n",
    "            \"extra_byte_size\": extra_byte_size,\n",
    "            \"actual_no_extra\": actual_no_extra,\n",
    "            \"second_bpp\": second_bpp,\n",
    "            \"bpp\": bpp\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "\n",
    "            print(\"Image dimensions: {}\".format(image_shape))\n",
    "            print(\"Theoretical size: {:.2f} bytes\".format(theoretical_byte_size))\n",
    "            print(\"Actual size: {:.2f} bytes\".format(actual_byte_size))\n",
    "            print(\"Extra information size: {:.2f} bytes {:.2f}% of actual size\".format(extra_byte_size, \n",
    "                                                                                       100 * extra_byte_size / actual_byte_size))\n",
    "            print(\"Actual size without extras: {:.2f} bytes\".format(actual_no_extra))\n",
    "            print(\"Efficiency: {:.3f}\".format(actual_byte_size / theoretical_byte_size))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"First level theoretical size: {:.2f} bytes\".format(first_level_theoretical))\n",
    "            print(\"First level actual (no extras) size: {:.2f} bytes\".format(first_level_actual_no_extra))\n",
    "            print(\"First level extras size: {:.2f} bytes\".format(first_level_extra))\n",
    "            print(\"First level Efficiency: {:.3f}\".format(\n",
    "                (first_level_actual_no_extra + first_level_extra) / first_level_theoretical))\n",
    "            \n",
    "            print(\"First level # of groups: {}\".format(len(group_indices1)))\n",
    "            print(\"First level greedy sample average log likelihood: {:.4f}\".format(first_level_avg_log_lik))\n",
    "            print(\"First level average sample log likelihood on level 1: {:.4f}\".format(first_level_sample_avg))\n",
    "            print(\"\")\n",
    "           \n",
    "            print(\"Second level theoretical size: {:.2f} bytes\".format(second_level_theoretical))\n",
    "            print(\"Second level actual (no extras) size: {:.2f} bytes\".format(second_level_actual_no_extra))\n",
    "            print(\"Second level extras size: {:.2f} bytes\".format(second_level_extra))\n",
    "\n",
    "            if use_importance_sampling:\n",
    "                print(\"{} outliers were not compressed (higher than {} bits of KL)\".format(len(outlier_extras2[0]),\n",
    "                                                                                           second_level_dim_kl_bit_limit))\n",
    "            print(\"Second level Efficiency: {:.3f}\".format(\n",
    "                (second_level_actual_no_extra + second_level_extra) / second_level_theoretical))\n",
    "            print(\"Second level's contribution to bpp: {:.4f}\".format(second_bpp))\n",
    "            print(\"Second level # of groups: {}\".format(len(group_indices2)))\n",
    "            print(\"Second level greedy sample average log likelihood: {:.4f}\".format(second_level_avg_log_lik))\n",
    "            print(\"Second level average sample log likelihood on level 1: {:.4f}\".format(second_level_sample_avg))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"{:.4f} bits / pixel\".format( bpp ))\n",
    "        \n",
    "        return (sample2, sample1), summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "q1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q1_loc\")).flatten()\n",
    "q1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q1_scale\")).flatten()\n",
    "\n",
    "p1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p1_loc\")).flatten()\n",
    "p1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p1_scale\")).flatten()\n",
    "\n",
    "q2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q2_loc\")).flatten()\n",
    "q2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q2_scale\")).flatten()\n",
    "\n",
    "p2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p2_loc\")).flatten()\n",
    "p2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p2_scale\")).flatten()\n",
    "\n",
    "q1 = tfd.Normal(loc=q1_loc,\n",
    "                scale=q1_scale)\n",
    "\n",
    "p1 = tfd.Normal(loc=p1_loc,\n",
    "                scale=p1_scale)\n",
    "\n",
    "q2 = tfd.Normal(loc=q2_loc,\n",
    "                scale=q2_scale)\n",
    "\n",
    "p2 = tfd.Normal(loc=p2_loc,\n",
    "                scale=p2_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateless_normal_sample(loc, scale, num_samples, seed):\n",
    "    \n",
    "    rank = tf.rank(loc)\n",
    "    sample_shape = tf.concat(([num_samples], tf.shape(loc)), axis=0)\n",
    "    tile_coefs = tf.concat(([num_samples], tf.tile([1], [rank])), axis=0)\n",
    "    \n",
    "    # Draw 0 mean, 1 variance samples\n",
    "    samples = tf.random.stateless_normal(shape=sample_shape, \n",
    "                                         seed=tf.concat(([seed], [42]), axis=0))\n",
    "    \n",
    "    # Transform them to the right thing by scaling and translating appropriately\n",
    "    samples = tf.tile(tf.expand_dims(scale, 0), tile_coefs) * samples\n",
    "    samples = tf.tile(tf.expand_dims(loc, 0), tile_coefs) + samples\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def code_importance_sample_(t_loc,\n",
    "                            t_scale,\n",
    "                            p_loc,\n",
    "                            p_scale,\n",
    "                            n_coding_bits,\n",
    "                            seed):\n",
    "    \n",
    "    \n",
    "    target=tfd.Normal(loc=t_loc,\n",
    "                      scale=t_scale)\n",
    "\n",
    "    proposal=tfd.Normal(loc=p_loc,\n",
    "                        scale=p_scale)\n",
    "    \n",
    "    #print(\"Taking {} samples per step\".format(n_samples))\n",
    "    \n",
    "    sample_index = []\n",
    "    \n",
    "    kls = tfd.kl_divergence(target, proposal)\n",
    "    total_kl = tf.reduce_sum(kls)\n",
    "    \n",
    "    num_samples = tf.cast(tf.ceil(tf.exp(total_kl)), tf.int32)\n",
    "    \n",
    "    # Set new seed\n",
    "    #samples = proposal.sample(num_samples, seed=seed) \n",
    "    samples = stateless_normal_sample(loc=p_loc, \n",
    "                                      scale=p_scale, \n",
    "                                      num_samples=num_samples, \n",
    "                                      seed=seed)\n",
    "\n",
    "    importance_weights = tf.reduce_sum(target.log_prob(samples) - proposal.log_prob(samples), axis=1)\n",
    "\n",
    "    index = tf.argmax(importance_weights)\n",
    "    best_sample = samples[index:index + 1, :]\n",
    "    \n",
    "    #index, best_sample = sess.run([idx, best_samp])\n",
    "    \n",
    "#     if np.log(index + 1) / np.log(2) > n_coding_bits:\n",
    "#         raise Exception(\"Not enough bits to code importance sample!\")\n",
    "    \n",
    "    # Turn the index into a bitstring\n",
    "    bitcode = tf.numpy_function(to_bit_string, [index, n_coding_bits], tf.string)\n",
    "\n",
    "    return best_sample, bitcode\n",
    "\n",
    "\n",
    "def decode_importance_sample_(sample_index, \n",
    "                              p_loc,\n",
    "                              p_scale,\n",
    "                              seed):\n",
    "\n",
    "    index = tf.numpy_function(from_bit_string, [sample_index], tf.int64)\n",
    "    \n",
    "    samples = stateless_normal_sample(loc=p_loc,\n",
    "                                      scale=p_scale,\n",
    "                                      num_samples=tf.cast(index, tf.int32) + 1,\n",
    "                                      seed=seed)\n",
    "    \n",
    "    return samples[index:, ...]\n",
    "\n",
    "\n",
    "def code_grouped_importance_sample_(sess,\n",
    "                                    target, \n",
    "                                    proposal, \n",
    "                                    seed,\n",
    "                                    n_bits_per_group,\n",
    "                                    max_group_size_bits=4,\n",
    "                                    dim_kl_bit_limit=12):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "        \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1])\n",
    "    t_scale = tf.reshape(target.scale / proposal.scale, [-1])\n",
    "    \n",
    "    # If we're going to do importance sampling, separate out dimensions with large KL,\n",
    "    # we'll deal with them separately.\n",
    "    kl_bits = tf.reshape(tfd.kl_divergence(target, proposal), [-1]) / np.log(2)\n",
    "\n",
    "    t_loc = sess.run(tf.where(kl_bits <= dim_kl_bit_limit, t_loc, p_loc))\n",
    "    t_scale = sess.run(tf.where(kl_bits <= dim_kl_bit_limit, t_scale, p_scale))\n",
    "\n",
    "    # We'll send the quantized samples for dimensions with high KL\n",
    "    outlier_indices = tf.where(kl_bits > dim_kl_bit_limit)\n",
    "\n",
    "    target_samples = tf.reshape(target.sample(), [-1])\n",
    "\n",
    "    # Select only the bits of the sample that are relevant\n",
    "    outlier_samples = tf.gather_nd(target_samples, outlier_indices)\n",
    "\n",
    "    # Halve precision\n",
    "    outlier_samples = tfq.quantize(outlier_samples, -30, 30, tf.quint16).output\n",
    "\n",
    "    outlier_extras = (outlier_indices, outlier_samples)\n",
    "    \n",
    "    kl_divergences = tf.reshape(\n",
    "        tfd.kl_divergence(tfd.Normal(loc=t_loc, scale=t_scale), \n",
    "                          tfd.Normal(loc=p_loc, scale=p_scale)), [-1])\n",
    "\n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "        \n",
    "    print(\"Maximum group KL: {:.3f}\".format(np.max(group_kls)))\n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    # Get the importance sampling op before looping it to avoid graph construction cost\n",
    "    # The length is variable, hence the shape is [None]\n",
    "    target_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    target_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    seed_feed = tf.placeholder(tf.int32)\n",
    "\n",
    "    result_ops = code_importance_sample_(t_loc=target_loc,\n",
    "                                         t_scale=target_scale,\n",
    "                                         p_loc=prop_loc,\n",
    "                                         p_scale=prop_scale,\n",
    "                                         seed=seed_feed,\n",
    "                                         n_coding_bits=n_bits_per_group)\n",
    "            \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        \n",
    "        result = sess.run(result_ops, feed_dict={target_loc: t_loc[start_idx:end_idx],\n",
    "                                                 target_scale: t_scale[start_idx:end_idx],\n",
    "                                                 prop_loc: p_loc[start_idx:end_idx],\n",
    "                                                 prop_scale: p_scale[start_idx:end_idx],\n",
    "                                                 seed_feed: seed + i\n",
    "                                                })\n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = tf.numpy_function(lambda code_words: ''.join([cw.decode(\"utf-8\") for cw in code_words]), \n",
    "                                [codes], \n",
    "                                tf.string)\n",
    "    \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = tf.where(kl_bits <= dim_kl_bit_limit, tf.squeeze(sample), target_samples)\n",
    "    \n",
    "    sample, bitcode, outlier_extras = sess.run([sample, bitcode, outlier_extras])\n",
    "    \n",
    "    return sample, bitcode, group_start_indices, outlier_extras\n",
    "\n",
    "\n",
    "def decode_grouped_importance_sample_(sess,\n",
    "                                     bitcode, \n",
    "                                     group_start_indices,\n",
    "                                     proposal, \n",
    "                                     n_bits_per_group,\n",
    "                                     seed,\n",
    "                                     outlier_indices,\n",
    "                                     outlier_samples):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "\n",
    "    # Placeholders\n",
    "    sample_index = tf.placeholder(tf.string)\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    seed_feed = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # Get decoding op\n",
    "    decode_op = decode_importance_sample_(sample_index=sample_index,\n",
    "                                          p_loc=prop_loc,\n",
    "                                          p_scale=prop_scale,\n",
    "                                          seed=seed_feed)\n",
    "\n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        samp = sess.run(decode_op, feed_dict = {\n",
    "            sample_index: bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            prop_loc: p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "            prop_scale: p_scale[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "            seed_feed: seed + i\n",
    "        })\n",
    "        \n",
    "        samples.append(samp)\n",
    "        \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    sample = tf.squeeze(sample)\n",
    "    \n",
    "    # Dequantize outliers\n",
    "    outlier_samples = tfq.dequantize(tf.cast(outlier_samples, tf.quint16), -30, 30)\n",
    "    \n",
    "    # Add back the quantized outliers\n",
    "    outlier_indices = tf.cast(tf.reshape(outlier_indices, [-1, 1]), tf.int32)\n",
    "    outlier_samples = tf.reshape(outlier_samples, [-1])\n",
    "    \n",
    "    updates = tf.scatter_nd(outlier_indices, \n",
    "                            outlier_samples, \n",
    "                            sample.shape.as_list())\n",
    "                            \n",
    "    sample = tf.where(tf.equal(updates, 0), sample, updates)\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_greedy_sample_(t_loc, \n",
    "                        t_scale,\n",
    "                        p_loc,\n",
    "                        p_scale,\n",
    "                        n_bits_per_step, \n",
    "                        n_steps, \n",
    "                        seed, \n",
    "                        rho=1.):\n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "\n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)\n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=p_loc / n_steps,\n",
    "                                scale=rho * p_scale / scale_divisor)\n",
    "    \n",
    "    target = tfd.Normal(loc=t_loc,\n",
    "                        scale=t_scale)\n",
    "\n",
    "    # Setup greedy sampler for loop\n",
    "    def loop_step(i, sample_index, best_sample):\n",
    "        samples = stateless_normal_sample(loc=proposal_shard.loc, \n",
    "                                          scale=proposal_shard.scale, \n",
    "                                          num_samples=n_samples, \n",
    "                                          seed=1000 * seed + i)\n",
    "        \n",
    "        test_samples = tf.tile(tf.expand_dims(best_sample, 0), [n_samples, 1]) + samples\n",
    "\n",
    "        log_probs = tf.reduce_sum(target.log_prob(test_samples), axis=1)\n",
    "\n",
    "        index = tf.argmax(log_probs)\n",
    "\n",
    "        best_sample = test_samples[index, :]\n",
    "\n",
    "        return [i + 1, tf.concat((sample_index, [index]), axis=0), best_sample]\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    best_sample = tf.zeros(tf.shape(p_loc), dtype=tf.float32)\n",
    "    sample_index = tf.cast([], tf.int32)\n",
    "    \n",
    "    cond = lambda i, sample_index, best_sample: i < n_steps\n",
    "\n",
    "    _, sample_index, best_sample = tf.while_loop(cond=cond,\n",
    "                                   body=loop_step, \n",
    "                                   loop_vars=[i, sample_index, best_sample],\n",
    "                                   shape_invariants=[i.get_shape(), \n",
    "                                                     tf.TensorShape([None]), \n",
    "                                                     best_sample.get_shape()])\n",
    "    \n",
    "    \n",
    "    sample_index = tf.map_fn(lambda x: tf.numpy_function(to_bit_string, [x, n_bits_per_step], tf.string), \n",
    "                             sample_index,\n",
    "                             dtype=tf.string)\n",
    "    \n",
    "    sample_index = tf.numpy_function(lambda indices: ''.join([ind.decode('utf-8') for ind in indices]),\n",
    "                                     [sample_index],\n",
    "                                     tf.string)\n",
    "    \n",
    "    return best_sample, sample_index\n",
    "\n",
    "\n",
    "\n",
    "def decode_greedy_sample_(sample_index, \n",
    "                          p_loc,\n",
    "                          p_scale,\n",
    "                          n_bits_per_step, \n",
    "                          n_steps, \n",
    "                          seed, \n",
    "                          rho=1.):\n",
    "    \n",
    "    \n",
    "    # Perform a for loop for the below list comprehension\n",
    "    #\n",
    "    #     indices = [from_bit_string(sample_index[i:i + n_bits_per_step]) \n",
    "    #                for i in range(0, n_bits_per_step * n_steps, n_bits_per_step)]\n",
    "    #\n",
    "    i = tf.constant(0, tf.int32)\n",
    "    indices = tf.cast([], tf.int32)\n",
    "    \n",
    "    cond = lambda i, indices: i < n_bits_per_step * n_steps\n",
    "\n",
    "    def index_loop_step(i, indices):\n",
    "        \n",
    "        index = tf.numpy_function(from_bit_string, \n",
    "                                  [tf.strings.substr(sample_index, i, n_bits_per_step)], \n",
    "                                  tf.int64)\n",
    "        \n",
    "        index = tf.cast(index, tf.int32)\n",
    "        \n",
    "        return [i + n_bits_per_step, tf.concat((indices, [index]), axis=0)]\n",
    "     \n",
    "    _, indices = tf.while_loop(cond=cond,\n",
    "                               body=index_loop_step, \n",
    "                               loop_vars=[i, indices],\n",
    "                               shape_invariants=[i.get_shape(), \n",
    "                                                 tf.TensorShape([None])])\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # Reconver the sample\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)    \n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=p_loc / n_steps,\n",
    "                                scale=rho * p_scale / scale_divisor)    \n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "    \n",
    "    # Loop variables\n",
    "    i = tf.constant(0, tf.int32)\n",
    "    sample = tf.zeros(tf.shape(p_loc), dtype=tf.float32)\n",
    "    \n",
    "    # Loop condition\n",
    "    cond = lambda i, indices: i < n_steps\n",
    "\n",
    "    # Loop body\n",
    "    def sample_loop_step(i, sample):\n",
    "        \n",
    "        samples = tf.tile(tf.expand_dims(sample, 0), [n_samples, 1])\n",
    "        \n",
    "        samples = samples + stateless_normal_sample(loc=proposal_shard.loc, \n",
    "                                                    scale=proposal_shard.scale, \n",
    "                                                    num_samples=n_samples, \n",
    "                                                    seed=1000 * seed + i)\n",
    "\n",
    "        return [i + 1, samples[indices[i], :]]\n",
    "     \n",
    "    # Run loop\n",
    "    _, sample = tf.while_loop(cond=cond,\n",
    "                              body=sample_loop_step, \n",
    "                              loop_vars=[i, sample],\n",
    "                              shape_invariants=[i.get_shape(), \n",
    "                                                sample.get_shape()])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def code_grouped_greedy_sample_(sess,\n",
    "                                target, \n",
    "                               proposal,\n",
    "                               n_steps, \n",
    "                               n_bits_per_step,\n",
    "                               seed,\n",
    "                               max_group_size_bits=12,\n",
    "                               adaptive=True,\n",
    "                               backfitting_steps=0,\n",
    "                               use_log_prob=False,\n",
    "                               rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = sess.run(tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1]))\n",
    "    t_scale = sess.run(tf.reshape(target.scale / proposal.scale, [-1]))\n",
    "    \n",
    "    kl_divergences = tf.reshape(tfd.kl_divergence(target, proposal), [-1])\n",
    "        \n",
    "    # ====================================================================== \n",
    "    # Preprocessing step: determine groups for sampling\n",
    "    # ====================================================================== \n",
    "\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "    \n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "            \n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    # Get the importance sampling op before looping it to avoid graph construction cost\n",
    "    # The length is variable, hence the shape is [None]\n",
    "    target_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    target_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    seed_feed = tf.placeholder(tf.int32)\n",
    "    \n",
    "    greedy_op = code_greedy_sample_(t_loc=target_loc,\n",
    "                                    t_scale=target_scale,\n",
    "                                    p_loc=prop_loc,\n",
    "                                    p_scale=prop_scale,\n",
    "                                    n_bits_per_step=n_bits_per_step,\n",
    "                                    n_steps=n_steps, \n",
    "                                    seed=seed_feed,\n",
    "                                    rho=rho)\n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        result = sess.run(greedy_op, feed_dict={target_loc: t_loc[start_idx:end_idx],\n",
    "                                                target_scale: t_scale[start_idx:end_idx],\n",
    "                                                prop_loc: p_loc[start_idx:end_idx],\n",
    "                                                prop_scale: p_scale[start_idx:end_idx],\n",
    "                                                seed_feed: seed + i})\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = ''.join([c.decode('utf-8') for c in codes])\n",
    "    sample = tf.concat(samples, axis=0)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = sess.run(sample)\n",
    "    \n",
    "    return sample, bitcode, group_start_indices\n",
    "  \n",
    "    \n",
    "def decode_grouped_greedy_sample_(sess,\n",
    "                                  bitcode, \n",
    "                                 group_start_indices,\n",
    "                                 proposal, \n",
    "                                 n_bits_per_step, \n",
    "                                 n_steps, \n",
    "                                 seed,\n",
    "                                 adaptive=True,\n",
    "                                 rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "    \n",
    "    # Placeholders\n",
    "    sample_index = tf.placeholder(tf.string)\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    seed_feed = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # Get decoding op\n",
    "    decode_greedy_op = decode_greedy_sample_(sample_index=sample_index, \n",
    "                                             p_loc=prop_loc,\n",
    "                                             p_scale=prop_scale,\n",
    "                                             n_bits_per_step=n_bits_per_step, \n",
    "                                             n_steps=n_steps, \n",
    "                                             seed=seed_feed, \n",
    "                                             rho=rho)\n",
    "\n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        if bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)] == '':\n",
    "            break\n",
    "        \n",
    "        samp = sess.run(decode_greedy_op, feed_dict = {\n",
    "            sample_index: bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            prop_loc: p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "            prop_scale: p_scale[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "            seed_feed: seed + i\n",
    "        })\n",
    "        \n",
    "        samples.append(samp)\n",
    "    \n",
    "        \n",
    "    sample = tf.concat(samples, axis=0)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total KL to split up: 4823000.94 bits, maximum bits per group: 420, estimated number of groups: 11484.0,coding 196608 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12862/12862 [09:14<00:00, 23.19it/s]\n",
      "100%|| 12857/12863 [03:20<00:00, 58.48it/s]"
     ]
    }
   ],
   "source": [
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 42\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "\n",
    "#     res = code_greedy_sample_(t_loc=q1_loc,\n",
    "#                               t_scale=q1_scale,\n",
    "#                               p_loc=p1_loc,\n",
    "#                               p_scale=p1_scale,\n",
    "#                                 n_bits_per_step=14, \n",
    "#                                 n_steps=30, \n",
    "#                                 seed=seed, \n",
    "#                                 rho=1.)\n",
    "    \n",
    "#     bs, si = sess.run(res)\n",
    "    \n",
    "#     res = decode_greedy_sample_(sample_index=si, \n",
    "#                                   p_loc=p1_loc,\n",
    "#                                   p_scale=p1_scale,\n",
    "#                                   n_bits_per_step=14, \n",
    "#                                   n_steps=30, \n",
    "#                                   seed=seed, \n",
    "#                                   rho=1.)\n",
    "#     samp = sess.run(res)\n",
    "\n",
    "    res = code_grouped_greedy_sample_(sess=sess,\n",
    "                                    target=q1, \n",
    "                                   proposal=p1,\n",
    "                                   n_steps=n_steps, \n",
    "                                   n_bits_per_step=n_bits_per_step,\n",
    "                                   seed=seed,\n",
    "                                   max_group_size_bits=12,\n",
    "                                   adaptive=True,\n",
    "                                   backfitting_steps=0,\n",
    "                                   use_log_prob=False,\n",
    "                                   rho=1.)\n",
    "    \n",
    "    sample, bitcode, group_start_indices = res\n",
    "    \n",
    "    dec = decode_grouped_greedy_sample_(sess=sess,\n",
    "                                  bitcode=bitcode, \n",
    "                                 group_start_indices=group_start_indices,\n",
    "                                 proposal=p1, \n",
    "                                 n_bits_per_step=n_bits_per_step, \n",
    "                                 n_steps=n_steps, \n",
    "                                 seed=seed,\n",
    "                                 adaptive=True,\n",
    "                                 rho=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.7369063 ,  1.7254381 ,  1.1242245 ,  1.1053375 ,  2.842863  ,\n",
       "       -0.14734206, -1.9815679 , -0.42015657, -2.9239683 ,  1.4816517 ,\n",
       "        1.3751918 ,  0.73930454,  1.2112267 ,  0.27395856,  1.832334  ,\n",
       "       -1.1119275 ,  0.28982404, -2.9413426 ,  0.29808068,  0.89563274,\n",
       "       -0.7213308 ,  2.3427196 ,  0.9718949 ,  0.20729369, -0.55134124,\n",
       "        2.0564656 , -2.6341462 , -2.0878568 , -1.7886444 ,  1.1009716 ,\n",
       "        1.8449075 , -1.0387864 ,  1.6496565 , -0.9662865 , -0.48589227,\n",
       "       -1.3502691 ,  2.1860561 ,  1.5237201 ,  1.131703  , -1.5085055 ,\n",
       "        1.6110754 , -0.4498502 ,  0.40226173,  2.5523753 , -0.55454504,\n",
       "       -0.4052822 ,  0.64688796,  0.34428656, -0.8174454 ,  0.82755554,\n",
       "        2.855883  , -1.2669371 ,  0.22776484, -0.60087276, -1.5215497 ,\n",
       "        0.9644736 , -1.4209671 , -2.6455266 , -0.9499182 , -2.5491168 ,\n",
       "        3.2639394 , -1.7531929 ,  1.2637122 ,  2.2786334 ,  0.2574938 ,\n",
       "        0.92850786,  1.0731263 , -0.81146634,  0.56138176,  0.04910831,\n",
       "       -0.68040395,  1.0261658 ,  0.21841016,  1.3265216 , -2.2232969 ,\n",
       "        0.603972  ,  1.498067  ,  0.21444887,  1.9787613 , -1.890182  ,\n",
       "        1.0159578 ,  0.3493658 , -2.3838744 ,  1.1632292 , -1.4158552 ,\n",
       "        2.009142  , -0.0586085 , -1.3922282 ,  1.1801078 , -1.8802583 ,\n",
       "        2.178896  ,  1.2693563 ,  1.2869782 ,  2.5009477 ,  1.3431032 ,\n",
       "       -0.8420969 , -0.6960917 ,  0.07054141,  2.6133757 , -0.03158572],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|| 12857/12863 [03:40<00:00, 58.48it/s]"
     ]
    }
   ],
   "source": [
    "np.all(sample == dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "pln_code_image_greedy(image=None, \n",
    "                      latent_dist_dir=latent_dist_dir,\n",
    "                      latent_dist_format=latent_dist_format,\n",
    "                      seed=seed, \n",
    "                      n_steps=n_steps,\n",
    "                      n_bits_per_step=n_bits_per_step,\n",
    "                      comp_file_path=comp_file_path,\n",
    "                      backfitting_steps_level_1=0,\n",
    "                      backfitting_steps_level_2=0,\n",
    "                      use_log_prob=False,\n",
    "                      rho=rho,\n",
    "                      use_importance_sampling=True,\n",
    "                      first_level_max_group_size_bits=first_level_max_group_size_bits,\n",
    "                      second_level_n_bits_per_group=20,\n",
    "                      second_level_max_group_size_bits=second_level_max_group_size_bits,\n",
    "                      second_level_dim_kl_bit_limit=12,\n",
    "                      outlier_index_bytes=3,\n",
    "                      outlier_sample_bytes=2,\n",
    "                      verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12006   457  7170 12006   457  8282  3499  6210  3870  8282  7170 11610\n",
      " 12732  6210   457  3499 12006   692  5645   330  2653 15744  3596  6210\n",
      " 12732  3168  8399 13460  7933  8399]\n"
     ]
    }
   ],
   "source": [
    "siz = tf.constant(4, dtype=tf.int32)\n",
    "\n",
    "r = tf.random.stateless_normal(shape=[siz] + [3], seed=[2, 8])\n",
    "\n",
    "\n",
    "i = tf.constant(0, tf.int32)\n",
    "indices = tf.cast([], tf.int32)\n",
    "\n",
    "cond = lambda i, indices: i < 14 * 30\n",
    "\n",
    "def index_loop_step(i, indices):\n",
    "\n",
    "    index = tf.numpy_function(from_bit_string, [tf.strings.substr(si, i, 14)], tf.int64)\n",
    "    index = tf.cast(index, tf.int32)\n",
    "\n",
    "    return [i + 14, tf.concat((indices, [index]), axis=0)]\n",
    "\n",
    "_, indices = tf.while_loop(cond=cond,\n",
    "                           body=index_loop_step, \n",
    "                           loop_vars=[i, indices],\n",
    "                           shape_invariants=[i.get_shape(), \n",
    "                                             tf.TensorShape([None])])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(sess.run(indices))\n",
    "    \n",
    "    #print(sess.run(tf.tile([1], [siz])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_68:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     ops = code_importance_sample_(t_loc=q2_loc,\n",
    "#                                   t_scale=q2_scale,\n",
    "#                                   p_loc=p2_loc,\n",
    "#                                   p_scale=p2_scale,\n",
    "#                                   n_coding_bits=20,\n",
    "#                                   seed=seed)\n",
    "    \n",
    "    \n",
    "#     best_sample, bitcode = sess.run(ops)\n",
    "    \n",
    "        \n",
    "#     sample_index = tf.placeholder(tf.string)\n",
    "#     samp_op = decode_importance_sample_(sample_index, proposal=p2, seed=seed)\n",
    "    \n",
    "#     samp = sess.run(samp_op, feed_dict={sample_index: bitcode})\n",
    "\n",
    "#     res = code_grouped_importance_sample_(sess=sess,\n",
    "#                                             target=q2, \n",
    "#                                             proposal=p2, \n",
    "#                                             seed=seed,\n",
    "#                                             n_bits_per_group=20,\n",
    "#                                             max_group_size_bits=4,\n",
    "#                                             dim_kl_bit_limit=12)\n",
    "    \n",
    "#     sample, bitcode, group_start_indices, outlier_extras = res\n",
    "    \n",
    "#     decoded = decode_grouped_importance_sample_(sess=sess,\n",
    "#                                                  bitcode=bitcode, \n",
    "#                                                  group_start_indices=group_start_indices,\n",
    "#                                                  proposal=p2, \n",
    "#                                                  n_bits_per_group=20,\n",
    "#                                                  seed=seed,\n",
    "#                                                  outlier_indices=outlier_extras[0],\n",
    "#                                                  outlier_samples=outlier_extras[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
