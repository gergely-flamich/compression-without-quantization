{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code\")\n",
    "sys.path.append(\"/homes/gf332/compression-without-quantization/code/thesis_code\")\n",
    "\n",
    "import os, glob\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfl = tf.keras.layers\n",
    "tfq = tf.quantization\n",
    "\n",
    "from binary_io import to_bit_string, from_bit_string\n",
    "\n",
    "from architectures import ProbabilisticLadderNetwork, VariationalAutoEncoder\n",
    "\n",
    "from miracle import create_dataset, quantize_image, read_png\n",
    "\n",
    "from greedy_compression import code_grouped_greedy_sample, code_grouped_importance_sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pln_code_image_greedy(image, \n",
    "                          latent_dist_dir,\n",
    "                          latent_dist_format,\n",
    "                          seed, \n",
    "                          n_steps,\n",
    "                          n_bits_per_step,\n",
    "                          comp_file_path,\n",
    "                          backfitting_steps_level_1=0,\n",
    "                          backfitting_steps_level_2=0,\n",
    "                          use_log_prob=False,\n",
    "                          rho=1.,\n",
    "                          use_importance_sampling=True,\n",
    "                          first_level_max_group_size_bits=12,\n",
    "                          second_level_n_bits_per_group=20,\n",
    "                          second_level_max_group_size_bits=4,\n",
    "                          second_level_dim_kl_bit_limit=12,\n",
    "                          outlier_index_bytes=3,\n",
    "                          outlier_sample_bytes=2,\n",
    "                          verbose=False):\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 1: Set the latent distributions for the image\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        q1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q1_loc\"))\n",
    "        q1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q1_scale\"))\n",
    "\n",
    "        p1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p1_loc\"))\n",
    "        p1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p1_scale\"))\n",
    "\n",
    "        q2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q2_loc\"))\n",
    "        q2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q2_scale\"))\n",
    "\n",
    "        p2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p2_loc\"))\n",
    "        p2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p2_scale\"))\n",
    "        \n",
    "        q1 = tfd.Normal(loc=q1_loc,\n",
    "                        scale=q1_scale)\n",
    "        \n",
    "        p1 = tfd.Normal(loc=p1_loc,\n",
    "                        scale=p1_scale)\n",
    "        \n",
    "        q2 = tfd.Normal(loc=q2_loc,\n",
    "                        scale=q2_scale)\n",
    "        \n",
    "        p2 = tfd.Normal(loc=p2_loc,\n",
    "                        scale=p2_scale)\n",
    "        \n",
    "        first_level_shape = q1.loc.shape.as_list()\n",
    "        second_level_shape = q2.loc.shape.as_list()\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 2: Create a coded sample of the latent space\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        if verbose: print(\"Coding second level\")\n",
    "            \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            sample2, code2, group_indices2, outlier_extras2 = code_grouped_importance_sample(\n",
    "                target=q2, \n",
    "                proposal=p2, \n",
    "                n_bits_per_group=second_level_n_bits_per_group, \n",
    "                seed=seed, \n",
    "                max_group_size_bits=second_level_max_group_size_bits,\n",
    "                dim_kl_bit_limit=second_level_dim_kl_bit_limit)\n",
    "            \n",
    "            outlier_extras2 = list(map(lambda x: tf.reshape(x, [-1]).numpy(), outlier_extras2))\n",
    "            \n",
    "        else:\n",
    "            sample2, code2, group_indices2 = code_grouped_greedy_sample(target=q2, \n",
    "                                                                        proposal=p2, \n",
    "                                                                        n_bits_per_step=n_bits_per_step, \n",
    "                                                                        n_steps=n_steps, \n",
    "                                                                        seed=seed, \n",
    "                                                                        max_group_size_bits=second_level_max_group_size_bits,\n",
    "                                                                        adaptive=True,\n",
    "                                                                        backfitting_steps=backfitting_steps_level_2,\n",
    "                                                                        use_log_prob=use_log_prob,\n",
    "                                                                        rho=rho)\n",
    "            \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences2 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices2)):\n",
    "            group_differences2.append(group_indices2[i] - group_indices2[i - 1])\n",
    "        \n",
    "        # We need to adjust the priors to the second stage sample\n",
    "        latents = (tf.reshape(sample2, second_level_shape), latents[1])\n",
    "        \n",
    "        \n",
    "        # Calculate the priors\n",
    "        self.decode(latents)\n",
    "        \n",
    "        if verbose: print(\"Coding first level\")\n",
    "            \n",
    "        sample1, code1, group_indices1 = code_grouped_greedy_sample(target=self.latent_posteriors[0], \n",
    "                                                                    proposal=self.latent_priors[0], \n",
    "                                                                    n_bits_per_step=n_bits_per_step, \n",
    "                                                                    n_steps=n_steps, \n",
    "                                                                    seed=seed, \n",
    "                                                                    max_group_size_bits=first_level_max_group_size_bits,\n",
    "                                                                    backfitting_steps=backfitting_steps_level_1,\n",
    "                                                                    use_log_prob=use_log_prob,\n",
    "                                                                    adaptive=True)\n",
    "        \n",
    "        # We will encode the group differences as this will cost us less\n",
    "        group_differences1 = [0]\n",
    "        \n",
    "        for i in range(1, len(group_indices1)):\n",
    "            group_differences1.append(group_indices1[i] - group_indices1[i - 1])\n",
    "        \n",
    "        bitcode = code1 + code2\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 3: Write the compressed file\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        extras = [seed, n_steps, n_bits_per_step] + first_level_shape[1:3] + second_level_shape[1:3]\n",
    "        \n",
    "        var_length_extras = [group_differences1, group_differences2]\n",
    "        var_length_bits = [first_level_max_group_size_bits,  \n",
    "                           second_level_max_group_size_bits]\n",
    "        \n",
    "        if use_importance_sampling:\n",
    "            \n",
    "            var_length_extras += outlier_extras2\n",
    "            var_length_bits += [ outlier_index_bytes * 8, outlier_sample_bytes * 8 ]\n",
    "    \n",
    "        write_bin_code(bitcode, \n",
    "                       comp_file_path, \n",
    "                       extras=extras,\n",
    "                       var_length_extras=var_length_extras,\n",
    "                       var_length_bits=var_length_bits)\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Step 4: Some logging information\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        total_kls = [tf.reduce_sum(x) for x in self.kl_divergence]\n",
    "        total_kl = sum(total_kls)\n",
    "\n",
    "        theoretical_byte_size = (total_kl + 2 * np.log(total_kl + 1)) / np.log(2) / 8\n",
    "        extra_byte_size = len(group_indices1) * var_length_bits[0] // 8 + \\\n",
    "                          len(group_indices2) * var_length_bits[1] // 8 + 7 * 2\n",
    "        actual_byte_size = os.path.getsize(comp_file_path)\n",
    "\n",
    "        actual_no_extra = actual_byte_size - extra_byte_size\n",
    "        \n",
    "        first_level_theoretical = (total_kls[0] + 2 * np.log(total_kls[0] + 1)) / np.log(2) / 8\n",
    "        first_level_actual_no_extra = len(code1) / 8\n",
    "        first_level_extra = len(group_indices1) * var_length_bits[0] // 8\n",
    "\n",
    "        sample1_reshaped = tf.reshape(sample1, first_level_shape)\n",
    "        first_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[0].log_prob(sample1_reshaped))\n",
    "        first_level_sample_avg = tf.reduce_mean(self.latent_posteriors[0].log_prob(self.latent_posteriors[0].sample()))\n",
    "        \n",
    "        second_level_theoretical = (total_kls[1] + 2 * np.log(total_kls[1] + 1)) / np.log(2) / 8\n",
    "        second_level_actual_no_extra = len(code2) / 8\n",
    "        second_level_extra = len(group_indices2) * var_length_bits[1] // 8 + 1\n",
    "        \n",
    "        second_bpp = (second_level_actual_no_extra + second_level_extra) * 8 / (image_shape[1] * image_shape[2]) \n",
    "\n",
    "        sample2_reshaped = tf.reshape(sample2, second_level_shape)\n",
    "        second_level_avg_log_lik = tf.reduce_mean(self.latent_posteriors[1].log_prob(sample2_reshaped))\n",
    "        second_level_sample_avg = tf.reduce_mean(self.latent_posteriors[1].log_prob(self.latent_posteriors[1].sample()))\n",
    "        \n",
    "        bpp = 8 * actual_byte_size / (image_shape[1] * image_shape[2]) \n",
    "        \n",
    "        summaries = {\n",
    "            \"image_shape\": image_shape,\n",
    "            \"theoretical_byte_size\": float(theoretical_byte_size.numpy()),\n",
    "            \"actual_byte_size\": actual_byte_size,\n",
    "            \"extra_byte_size\": extra_byte_size,\n",
    "            \"actual_no_extra\": actual_no_extra,\n",
    "            \"second_bpp\": second_bpp,\n",
    "            \"bpp\": bpp\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "\n",
    "            print(\"Image dimensions: {}\".format(image_shape))\n",
    "            print(\"Theoretical size: {:.2f} bytes\".format(theoretical_byte_size))\n",
    "            print(\"Actual size: {:.2f} bytes\".format(actual_byte_size))\n",
    "            print(\"Extra information size: {:.2f} bytes {:.2f}% of actual size\".format(extra_byte_size, \n",
    "                                                                                       100 * extra_byte_size / actual_byte_size))\n",
    "            print(\"Actual size without extras: {:.2f} bytes\".format(actual_no_extra))\n",
    "            print(\"Efficiency: {:.3f}\".format(actual_byte_size / theoretical_byte_size))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"First level theoretical size: {:.2f} bytes\".format(first_level_theoretical))\n",
    "            print(\"First level actual (no extras) size: {:.2f} bytes\".format(first_level_actual_no_extra))\n",
    "            print(\"First level extras size: {:.2f} bytes\".format(first_level_extra))\n",
    "            print(\"First level Efficiency: {:.3f}\".format(\n",
    "                (first_level_actual_no_extra + first_level_extra) / first_level_theoretical))\n",
    "            \n",
    "            print(\"First level # of groups: {}\".format(len(group_indices1)))\n",
    "            print(\"First level greedy sample average log likelihood: {:.4f}\".format(first_level_avg_log_lik))\n",
    "            print(\"First level average sample log likelihood on level 1: {:.4f}\".format(first_level_sample_avg))\n",
    "            print(\"\")\n",
    "           \n",
    "            print(\"Second level theoretical size: {:.2f} bytes\".format(second_level_theoretical))\n",
    "            print(\"Second level actual (no extras) size: {:.2f} bytes\".format(second_level_actual_no_extra))\n",
    "            print(\"Second level extras size: {:.2f} bytes\".format(second_level_extra))\n",
    "\n",
    "            if use_importance_sampling:\n",
    "                print(\"{} outliers were not compressed (higher than {} bits of KL)\".format(len(outlier_extras2[0]),\n",
    "                                                                                           second_level_dim_kl_bit_limit))\n",
    "            print(\"Second level Efficiency: {:.3f}\".format(\n",
    "                (second_level_actual_no_extra + second_level_extra) / second_level_theoretical))\n",
    "            print(\"Second level's contribution to bpp: {:.4f}\".format(second_bpp))\n",
    "            print(\"Second level # of groups: {}\".format(len(group_indices2)))\n",
    "            print(\"Second level greedy sample average log likelihood: {:.4f}\".format(second_level_avg_log_lik))\n",
    "            print(\"Second level average sample log likelihood on level 1: {:.4f}\".format(second_level_sample_avg))\n",
    "            print(\"\")\n",
    "            \n",
    "            print(\"{:.4f} bits / pixel\".format( bpp ))\n",
    "        \n",
    "        return (sample2, sample1), summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "q1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q1_loc\")).flatten()[:100]\n",
    "q1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q1_scale\")).flatten()[:100]\n",
    "\n",
    "p1_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p1_loc\")).flatten()[:100]\n",
    "p1_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p1_scale\")).flatten()[:100]\n",
    "\n",
    "q2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"q2_loc\")).flatten()\n",
    "q2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"q2_scale\")).flatten()\n",
    "\n",
    "p2_loc = np.load(latent_dist_dir + latent_dist_format.format(\"p2_loc\")).flatten()\n",
    "p2_scale = np.load(latent_dist_dir + latent_dist_format.format(\"p2_scale\")).flatten()\n",
    "\n",
    "q1 = tfd.Normal(loc=q1_loc,\n",
    "                scale=q1_scale)\n",
    "\n",
    "p1 = tfd.Normal(loc=p1_loc,\n",
    "                scale=p1_scale)\n",
    "\n",
    "q2 = tfd.Normal(loc=q2_loc,\n",
    "                scale=q2_scale)\n",
    "\n",
    "p2 = tfd.Normal(loc=p2_loc,\n",
    "                scale=p2_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_importance_sample_(t_loc,\n",
    "                            t_scale,\n",
    "                            p_loc,\n",
    "                            p_scale,\n",
    "                            n_coding_bits,\n",
    "                            seed):\n",
    "    \n",
    "    \n",
    "    target=tfd.Normal(loc=t_loc,\n",
    "                      scale=t_scale)\n",
    "\n",
    "    proposal=tfd.Normal(loc=p_loc,\n",
    "                        scale=p_scale)\n",
    "        \n",
    "    rank = len(proposal.loc.shape.as_list())\n",
    "    \n",
    "    #print(\"Taking {} samples per step\".format(n_samples))\n",
    "    \n",
    "    sample_index = []\n",
    "    \n",
    "    kls = tfd.kl_divergence(target, proposal)\n",
    "    total_kl = tf.reduce_sum(kls)\n",
    "    \n",
    "    num_samples = tf.cast(tf.ceil(tf.exp(total_kl)), tf.int32)\n",
    "    \n",
    "    # Set new seed\n",
    "    #samples = proposal.sample(num_samples, seed=seed)\n",
    "    \n",
    "    # Draw 0 mean, 1 variance samples\n",
    "    samples = tf.random.stateless_normal(shape=[num_samples] + t_loc.shape.as_list(), \n",
    "                                         seed=tf.Variable([1, 42]))\n",
    "    \n",
    "    # Transform them to the right thing by scaling and translating appropriately\n",
    "    samples = tf.tile(tf.expand_dims(p_scale, 0), [num_samples] + [1] * rank) * samples\n",
    "    samples = tf.tile(tf.expand_dims(p_loc, 0), [num_samples] + [1] * rank) + samples\n",
    "\n",
    "    importance_weights = tf.reduce_sum(target.log_prob(samples) - proposal.log_prob(samples), axis=1)\n",
    "\n",
    "    index = tf.argmax(importance_weights)\n",
    "    best_sample = samples[index:index + 1, :]\n",
    "    \n",
    "    #index, best_sample = sess.run([idx, best_samp])\n",
    "    \n",
    "#     if np.log(index + 1) / np.log(2) > n_coding_bits:\n",
    "#         raise Exception(\"Not enough bits to code importance sample!\")\n",
    "    \n",
    "    # Turn the index into a bitstring\n",
    "    bitcode = tf.numpy_function(to_bit_string, [index, n_coding_bits], tf.string)\n",
    "\n",
    "    return best_sample, bitcode\n",
    "\n",
    "\n",
    "def decode_importance_sample_(sample_index, \n",
    "                              p_loc,\n",
    "                              p_scale,\n",
    "                              seed):\n",
    "    \n",
    "    proposal = tfd.Normal(loc=p_loc,\n",
    "                          scale=p_scale)\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    index = tf.numpy_function(from_bit_string, [sample_index], tf.int64)\n",
    "    \n",
    "    #tf.random.set_random_seed(seed)\n",
    "    samples = proposal.sample(tf.cast(index, tf.int32) + 1, seed=seed)\n",
    "    \n",
    "    return samples[index:, ...]\n",
    "\n",
    "\n",
    "def code_grouped_importance_sample_(sess,\n",
    "                                    target, \n",
    "                                    proposal, \n",
    "                                    seed,\n",
    "                                    n_bits_per_group,\n",
    "                                    max_group_size_bits=4,\n",
    "                                    dim_kl_bit_limit=12):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "        \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1])\n",
    "    t_scale = tf.reshape(target.scale / proposal.scale, [-1])\n",
    "    \n",
    "    # If we're going to do importance sampling, separate out dimensions with large KL,\n",
    "    # we'll deal with them separately.\n",
    "    kl_bits = tf.reshape(tfd.kl_divergence(target, proposal), [-1]) / np.log(2)\n",
    "\n",
    "    t_loc = sess.run(tf.where(kl_bits <= dim_kl_bit_limit, t_loc, p_loc))\n",
    "    t_scale = sess.run(tf.where(kl_bits <= dim_kl_bit_limit, t_scale, p_scale))\n",
    "\n",
    "    # We'll send the quantized samples for dimensions with high KL\n",
    "    outlier_indices = tf.where(kl_bits > dim_kl_bit_limit)\n",
    "\n",
    "    target_samples = tf.reshape(target.sample(), [-1])\n",
    "\n",
    "    # Select only the bits of the sample that are relevant\n",
    "    outlier_samples = tf.gather_nd(target_samples, outlier_indices)\n",
    "\n",
    "    # Halve precision\n",
    "    outlier_samples = tfq.quantize(outlier_samples, -30, 30, tf.quint16).output\n",
    "\n",
    "    outlier_extras = (outlier_indices, outlier_samples)\n",
    "    \n",
    "    kl_divergences = tf.reshape(\n",
    "        tfd.kl_divergence(tfd.Normal(loc=t_loc, scale=t_scale), \n",
    "                          tfd.Normal(loc=p_loc, scale=p_scale)), [-1])\n",
    "\n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "        \n",
    "    print(\"Maximum group KL: {:.3f}\".format(np.max(group_kls)))\n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    # Get the importance sampling op before looping it to avoid graph construction cost\n",
    "    # The length is variable, hence the shape is [None]\n",
    "    target_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    target_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    seed_feed = tf.placeholder(tf.int32, shape=[2])\n",
    "\n",
    "    result_ops = code_importance_sample_(t_loc=target_loc,\n",
    "                                         t_scale=target_scale,\n",
    "                                         p_loc=prop_loc,\n",
    "                                         p_scale=prop_scale,\n",
    "                                         seed=seed_feed,\n",
    "                                         n_coding_bits=n_bits_per_group)\n",
    "            \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        \n",
    "        result = sess.run(result_ops, feed_dict={target_loc: t_loc[start_idx:end_idx],\n",
    "                                                 target_scale: t_scale[start_idx:end_idx],\n",
    "                                                 prop_loc: p_loc[start_idx:end_idx],\n",
    "                                                 prop_scale: p_scale[start_idx:end_idx],\n",
    "                                                 seed_feed: [42, seed + i]\n",
    "                                                })\n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = tf.numpy_function(lambda code_words: ''.join([cw.decode(\"utf-8\") for cw in code_words]), \n",
    "                                [codes], \n",
    "                                tf.string)\n",
    "    \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = tf.where(kl_bits <= dim_kl_bit_limit, tf.squeeze(sample), target_samples)\n",
    "    \n",
    "    sample, bitcode, outlier_extras = sess.run([sample, bitcode, outlier_extras])\n",
    "    \n",
    "    return sample, bitcode, group_start_indices, outlier_extras\n",
    "\n",
    "\n",
    "def decode_grouped_importance_sample_(sess,\n",
    "                                     bitcode, \n",
    "                                     group_start_indices,\n",
    "                                     proposal, \n",
    "                                     n_bits_per_group,\n",
    "                                     seed,\n",
    "                                     outlier_indices,\n",
    "                                     outlier_samples):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = sess.run(tf.reshape(tf.zeros_like(proposal.loc), [-1]))\n",
    "    p_scale = sess.run(tf.reshape(tf.ones_like(proposal.scale), [-1]))\n",
    "\n",
    "    # Placeholders\n",
    "    sample_index = tf.placeholder(tf.string)\n",
    "    \n",
    "    prop_loc = tf.placeholder(tf.float32, shape=[None])\n",
    "    prop_scale = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    # Get decoding op\n",
    "    decode_op = decode_importance_sample_(sample_index=sample_index,\n",
    "                                          p_loc=prop_loc,\n",
    "                                          p_scale=prop_scale,\n",
    "                                          seed=seed)\n",
    "\n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        samp = sess.run(decode_op, feed_dict = {\n",
    "            sample_index: bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            prop_loc: p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "            prop_scale: p_scale[group_start_indices[i]:group_start_indices[i + 1]]\n",
    "        })\n",
    "        \n",
    "        samples.append(samp)\n",
    "        \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    sample = tf.squeeze(sample)\n",
    "    \n",
    "    # Dequantize outliers\n",
    "    outlier_samples = tfq.dequantize(tf.cast(outlier_samples, tf.quint16), -30, 30)\n",
    "    \n",
    "    # Add back the quantized outliers\n",
    "    outlier_indices = tf.cast(tf.reshape(outlier_indices, [-1, 1]), tf.int32)\n",
    "    outlier_samples = tf.reshape(outlier_samples, [-1])\n",
    "    \n",
    "    updates = tf.scatter_nd(outlier_indices, \n",
    "                            outlier_samples, \n",
    "                            sample.shape.as_list())\n",
    "                            \n",
    "    sample = tf.where(tf.equal(updates, 0), sample, updates)\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total KL to split up: 22277.49 bits, maximum bits per group: 20, estimated number of groups: 1114.0,coding 12288 dimensions\n",
      "Maximum group KL: 18.556\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7710abdeae55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                             \u001b[0mn_bits_per_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                             \u001b[0mmax_group_size_bits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                             dim_kl_bit_limit=12)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_start_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_extras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-905534aa2d7a>\u001b[0m in \u001b[0;36mcode_grouped_importance_sample_\u001b[0;34m(sess, target, proposal, seed, n_bits_per_group, max_group_size_bits, dim_kl_bit_limit)\u001b[0m\n\u001b[1;32m    185\u001b[0m                                          \u001b[0mp_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprop_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                                          \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_feed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                                          n_coding_bits=n_bits_per_group)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_start_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-905534aa2d7a>\u001b[0m in \u001b[0;36mcode_importance_sample_\u001b[0;34m(t_loc, t_scale, p_loc, p_scale, n_coding_bits, seed)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Draw 0 mean, 1 variance samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     samples = tf.random.stateless_normal(shape=[num_samples] + t_loc.shape.as_list(), \n\u001b[0;32m---> 31\u001b[0;31m                                          seed=tf.Variable([1, 42]))\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Transform them to the right thing by scaling and translating appropriately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/ops/stateless_random_ops.py\u001b[0m in \u001b[0;36mstateless_random_normal\u001b[0;34m(shape, seed, mean, stddev, dtype, name)\u001b[0m\n\u001b[1;32m    132\u001b[0m   with ops.name_scope(name, \"stateless_random_normal\",\n\u001b[1;32m    133\u001b[0m                       [shape, seed, mean, stddev]) as name:\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ShapeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stddev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36m_ShapeTensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1085\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1086\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1087\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;31m# convertible-to-tensor types, such as numpy arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m           elems_as_tensors.append(\n\u001b[0;32m-> 1094\u001b[0;31m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0m\u001b[1;32m   1095\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/scratch/gf332/compression_venv/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "#     ops = code_importance_sample_(t_loc=q2_loc,\n",
    "#                                   t_scale=q2_scale,\n",
    "#                                   p_loc=p2_loc,\n",
    "#                                   p_scale=p2_scale,\n",
    "#                                   n_coding_bits=20,\n",
    "#                                   seed=seed)\n",
    "    \n",
    "    \n",
    "#     best_sample, bitcode = sess.run(ops)\n",
    "    \n",
    "        \n",
    "#     sample_index = tf.placeholder(tf.string)\n",
    "#     samp_op = decode_importance_sample_(sample_index, proposal=p2, seed=seed)\n",
    "    \n",
    "#     samp = sess.run(samp_op, feed_dict={sample_index: bitcode})\n",
    "\n",
    "    res = code_grouped_importance_sample_(sess=sess,\n",
    "                                            target=q2, \n",
    "                                            proposal=p2, \n",
    "                                            seed=seed,\n",
    "                                            n_bits_per_group=20,\n",
    "                                            max_group_size_bits=4,\n",
    "                                            dim_kl_bit_limit=12)\n",
    "    \n",
    "    sample, bitcode, group_start_indices, outlier_extras = res\n",
    "    \n",
    "#     decoded = decode_grouped_importance_sample_(sess=sess,\n",
    "#                                                  bitcode=bitcode, \n",
    "#                                                  group_start_indices=group_start_indices,\n",
    "#                                                  proposal=p2, \n",
    "#                                                  n_bits_per_group=20,\n",
    "#                                                  seed=seed,\n",
    "#                                                  outlier_indices=outlier_extras[0],\n",
    "#                                                  outlier_samples=outlier_extras[1])\n",
    "\n",
    "#     res = code_grouped_greedy_sample_(sess=sess,\n",
    "#                                     target=q1, \n",
    "#                                    proposal=p1,\n",
    "#                                    n_steps=n_steps, \n",
    "#                                    n_bits_per_step=n_bits_per_step,\n",
    "#                                    seed=seed,\n",
    "#                                    max_group_size_bits=12,\n",
    "#                                    adaptive=True,\n",
    "#                                    backfitting_steps=0,\n",
    "#                                    use_log_prob=False,\n",
    "#                                    rho=1.)\n",
    "    \n",
    "#     sample, bitcode, group_start_indices = res\n",
    "    \n",
    "#     dec = decode_grouped_greedy_sample_(sess=sess,\n",
    "#                                   bitcode=bitcode, \n",
    "#                                  group_start_indices=group_start_indices,\n",
    "#                                  proposal=p1, \n",
    "#                                  n_bits_per_step=n_bits_per_step, \n",
    "#                                  n_steps=n_steps, \n",
    "#                                  seed=seed,\n",
    "#                                  adaptive=True,\n",
    "#                                  rho=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001891787"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001891787"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist_dir = \"/scratch/gf332/data/kodak_cwoq/\"\n",
    "latent_dist_format = \"pln_{}.npy\"\n",
    "\n",
    "comp_file_path = \"/scratch/gf332/data/kodak_cwoq/test.miracle\"\n",
    "\n",
    "n_bits_per_step = 14\n",
    "n_steps = 30\n",
    "seed = 1\n",
    "rho = 1.\n",
    "first_level_max_group_size_bits=12\n",
    "second_level_max_group_size_bits=4\n",
    "\n",
    "pln_code_image_greedy(image=None, \n",
    "                      latent_dist_dir=latent_dist_dir,\n",
    "                      latent_dist_format=latent_dist_format,\n",
    "                      seed=seed, \n",
    "                      n_steps=n_steps,\n",
    "                      n_bits_per_step=n_bits_per_step,\n",
    "                      comp_file_path=comp_file_path,\n",
    "                      backfitting_steps_level_1=0,\n",
    "                      backfitting_steps_level_2=0,\n",
    "                      use_log_prob=False,\n",
    "                      rho=rho,\n",
    "                      use_importance_sampling=True,\n",
    "                      first_level_max_group_size_bits=first_level_max_group_size_bits,\n",
    "                      second_level_n_bits_per_group=20,\n",
    "                      second_level_max_group_size_bits=second_level_max_group_size_bits,\n",
    "                      second_level_dim_kl_bit_limit=12,\n",
    "                      outlier_index_bytes=3,\n",
    "                      outlier_sample_bytes=2,\n",
    "                      verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_greedy_sample_(sess,\n",
    "                       target, \n",
    "                       proposal, \n",
    "                       n_bits_per_step, \n",
    "                       n_steps, \n",
    "                       seed, \n",
    "                       rho=1., \n",
    "                       backfitting_steps=0,\n",
    "                       use_log_prob=False):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "    \n",
    "    #print(\"Taking {} samples per step\".format(n_samples))\n",
    "\n",
    "    best_sample = tf.Variable(tf.zeros((1, dim), dtype=tf.float32))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_index = []\n",
    "    \n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)\n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=proposal.loc / n_steps,\n",
    "                                scale=rho * proposal.scale / scale_divisor)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # Set new seed\n",
    "        samples = proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "\n",
    "        test_samples = tf.tile(best_sample, [n_samples, 1]) + samples\n",
    "\n",
    "        log_probs = tf.reduce_sum(target.log_prob(test_samples), axis=1)\n",
    "\n",
    "        index = tf.argmax(log_probs)\n",
    "\n",
    "        update_samp_op = best_sample.assign(test_samples[index:index + 1, :])\n",
    "\n",
    "        idx, _ = sess.run([index, update_samp_op])\n",
    "        \n",
    "        sample_index.append(idx)\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Perform backfitting\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # TODO\n",
    "#     for b in range(backfitting_steps):\n",
    "        \n",
    "#         # Single backfitting step\n",
    "#         for i in range(n_steps):\n",
    "\n",
    "#             # Set seed to regenerate the previously generated samples here\n",
    "#             samples = proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "            \n",
    "#             idx = sample_index[i]\n",
    "            \n",
    "#             # Undo the addition of the current sample\n",
    "#             best_sample.assign_sub(samples[idx : idx + 1, :])\n",
    "            \n",
    "#             # Generate candidate samples\n",
    "#             test_samples = tf.tile(best_sample, [n_samples, 1]) + samples\n",
    "\n",
    "#             if use_log_prob:\n",
    "#                 test_scores = tf.reduce_sum(target.log_prob(test_samples), axis=1)\n",
    "#             else:\n",
    "#                 test_scores = tf.reduce_sum(-((test_samples - target.loc) / target.scale)**8,\n",
    "#                                            axis=1)\n",
    "\n",
    "#             index = tf.argmax(test_scores)\n",
    "\n",
    "#             samp_update_op = best_sample.assign(test_samples[index:index + 1, :])\n",
    "\n",
    "#             idx, _ = sess.run([index, samp_update_op])\n",
    "            \n",
    "#             sample_index[i] = idx\n",
    "    \n",
    "    \n",
    "    sample_index = list(map(lambda x: to_bit_string(x, n_bits_per_step), sample_index))\n",
    "    sample_index = ''.join(sample_index)\n",
    "    \n",
    "    return best_sample.eval(session=sess), sample_index\n",
    "\n",
    "\n",
    "\n",
    "def decode_greedy_sample_(sess, sample_index, proposal, n_bits_per_step, n_steps, seed, rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "        \n",
    "    dim = proposal.loc.shape.as_list()[0]\n",
    "    \n",
    "    indices = [from_bit_string(sample_index[i:i + n_bits_per_step]) \n",
    "               for i in range(0, n_bits_per_step * n_steps, n_bits_per_step)]\n",
    "        \n",
    "    # The scale divisor needs to be square rooted because\n",
    "    # we are dealing with standard deviations and not variances\n",
    "    scale_divisor = np.sqrt(n_steps)    \n",
    "    \n",
    "    proposal_shard = tfd.Normal(loc=proposal.loc / n_steps,\n",
    "                                scale=rho * proposal.scale / scale_divisor)    \n",
    "    \n",
    "    n_samples = int(2**n_bits_per_step)\n",
    "    \n",
    "    sample = tf.Variable(tf.zeros((1, dim), dtype=tf.float32))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        # Set new seed\n",
    "        samples = tf.tile(sample, [n_samples, 1]) + proposal_shard.sample(n_samples, seed=1000 * seed + i)\n",
    "\n",
    "        index = indices[i]\n",
    "\n",
    "        samp_update_op = sample.assign(samples[index:index + 1, :])\n",
    "        \n",
    "        sess.run(samp_update_op)\n",
    "    \n",
    "    return sample.eval(session=sess)\n",
    "\n",
    "\n",
    "def code_grouped_greedy_sample_(sess,\n",
    "                                target, \n",
    "                               proposal,\n",
    "                               n_steps, \n",
    "                               n_bits_per_step,\n",
    "                               seed,\n",
    "                               max_group_size_bits=12,\n",
    "                               adaptive=True,\n",
    "                               backfitting_steps=0,\n",
    "                               use_log_prob=False,\n",
    "                               rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if target.dtype is not tf.float32:\n",
    "        raise Exception(\"Target datatype must be float32!\")\n",
    "        \n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # rescale proposal by the proposal\n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "    \n",
    "    # rescale target by the proposal\n",
    "    t_loc = tf.reshape((target.loc - proposal.loc) / proposal.scale, [-1])\n",
    "    t_scale = tf.reshape(target.scale / proposal.scale, [-1])\n",
    "    \n",
    "    kl_divergences = tf.reshape(tfd.kl_divergence(target, proposal), [-1])\n",
    "        \n",
    "    # ====================================================================== \n",
    "    # Preprocessing step: determine groups for sampling\n",
    "    # ====================================================================== \n",
    "\n",
    "    group_start_indices = [0]\n",
    "    group_kls = []\n",
    "    \n",
    "    kl_divs = sess.run(kl_divergences)\n",
    "\n",
    "    total_kl_bits = np.sum(kl_divs) / np.log(2)\n",
    "\n",
    "    print(\"Total KL to split up: {:.2f} bits, \"\n",
    "          \"maximum bits per group: {}, \"\n",
    "          \"estimated number of groups: {},\"\n",
    "          \"coding {} dimensions\".format(total_kl_bits, \n",
    "                                        n_bits_per_group,\n",
    "                                        total_kl_bits // n_bits_per_group + 1,\n",
    "                                        num_dimensions\n",
    "                                        ))\n",
    "\n",
    "    current_group_size = 0\n",
    "    current_group_kl = 0\n",
    "    \n",
    "    n_nats_per_group = n_bits_per_group * np.log(2) - 1\n",
    "\n",
    "    for idx in range(num_dimensions):\n",
    "\n",
    "        group_bits = np.log(current_group_size + 1) / np.log(2)\n",
    "        \n",
    "        if group_bits >= max_group_size_bits or \\\n",
    "           current_group_kl + kl_divs[idx] >= n_nats_per_group or \\\n",
    "           idx == num_dimensions - 1:\n",
    "\n",
    "            group_start_indices.append(idx)\n",
    "            group_kls.append(current_group_kl / np.log(2))\n",
    "\n",
    "            current_group_size = 1\n",
    "            current_group_kl = kl_divs[idx]\n",
    "            \n",
    "        else:\n",
    "            current_group_kl += kl_divs[idx]\n",
    "            current_group_size += 1\n",
    "            \n",
    "    # ====================================================================== \n",
    "    # Sample each group\n",
    "    # ====================================================================== \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions] \n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        start_idx = group_start_indices[i]\n",
    "        end_idx = group_start_indices[i + 1]\n",
    "        \n",
    "        result = code_greedy_sample_(\n",
    "            sess=sess,\n",
    "            target=tfd.Normal(loc=t_loc[start_idx:end_idx],\n",
    "                              scale=t_scale[start_idx:end_idx]), \n",
    "\n",
    "            proposal=tfd.Normal(loc=p_loc[start_idx:end_idx],\n",
    "                                scale=p_scale[start_idx:end_idx]), \n",
    "\n",
    "            n_bits_per_step=n_bits_per_step, \n",
    "            n_steps=n_steps, \n",
    "            seed=i + seed,\n",
    "            backfitting_steps=backfitting_steps,\n",
    "            use_log_prob=use_log_prob,\n",
    "            rho=rho)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "    samples, codes = zip(*results)\n",
    "    \n",
    "    bitcode = ''.join(codes)\n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    sample = sess.run(sample)\n",
    "    \n",
    "    return sample, bitcode, group_start_indices\n",
    "  \n",
    "    \n",
    "def decode_grouped_greedy_sample_(sess,\n",
    "                                  bitcode, \n",
    "                                 group_start_indices,\n",
    "                                 proposal, \n",
    "                                 n_bits_per_step, \n",
    "                                 n_steps, \n",
    "                                 seed,\n",
    "                                 adaptive=True,\n",
    "                                 rho=1.):\n",
    "    \n",
    "    # Make sure the distributions have the correct type\n",
    "    if proposal.dtype is not tf.float32:\n",
    "        raise Exception(\"Proposal datatype must be float32!\")\n",
    "    \n",
    "    n_bits_per_group = n_bits_per_step * n_steps\n",
    "    \n",
    "    num_dimensions = np.prod(proposal.loc.shape.as_list())\n",
    "    \n",
    "    # ====================================================================== \n",
    "    # Decode each group\n",
    "    # ====================================================================== \n",
    "                \n",
    "    samples = []\n",
    "    \n",
    "    group_start_indices += [num_dimensions]\n",
    "    \n",
    "    p_loc = tf.reshape(tf.zeros_like(proposal.loc), [-1])\n",
    "    p_scale = tf.reshape(tf.ones_like(proposal.scale), [-1])\n",
    "    \n",
    "    for i in tqdm(range(len(group_start_indices) - 1)):\n",
    "        \n",
    "        samples.append(decode_greedy_sample_(\n",
    "            sess=sess,\n",
    "            sample_index=bitcode[n_bits_per_group * i: n_bits_per_group * (i + 1)],\n",
    "            \n",
    "            proposal=tfd.Normal(loc=p_loc[group_start_indices[i]:group_start_indices[i + 1]],\n",
    "                                scale=p_scale[group_start_indices[i]:group_start_indices[i + 1]]), \n",
    "            \n",
    "            n_bits_per_step=n_bits_per_step, \n",
    "            n_steps=n_steps, \n",
    "            seed=i + seed,\n",
    "            rho=rho))\n",
    "        \n",
    "    sample = tf.concat(samples, axis=1)\n",
    "    \n",
    "    # Rescale the sample\n",
    "    sample = tf.reshape(proposal.scale, [-1]) * sample + tf.reshape(proposal.loc, [-1])\n",
    "    \n",
    "    return sess.run(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3804178 1.9624459 0.2679907]\n",
      " [1.3804178 1.9624459 0.2679907]]\n"
     ]
    }
   ],
   "source": [
    "r = tf.random.stateless_normal(shape=[3], seed=[2, 8])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(sess.run(tf.tile(tf.expand_dims(r, 0), [2,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
